{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b242297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.3.12, the latest is 0.4.0.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "# from passivefeatureslite.extract.applications import *\n",
    "from passivefeatureslite.extract.locations import *\n",
    "from passivefeatureslite.extract.bluetooth import *\n",
    "from passivefeatureslite.extract.call import *\n",
    "from passivefeatureslite.extract.message import *\n",
    "from passivefeatureslite.extract.wifi import *\n",
    "from passivefeatureslite.extract.screen import *\n",
    "\n",
    "from visualutils import plot_one_col\n",
    "from syncutils import get_sensor_samples_per_survey_measure\n",
    "from statsutils import norm_per_person_wrapper, cut_dates, get_95_ci_lower, get_95_ci_upper, get_95_ci_med_lower, get_95_ci_med_upper\n",
    "from timesettings import COVID_CUT_OFF, LOCKDOWN_YELLOW_START, LOCKDOWN_GREEN_START\n",
    "\n",
    "from timeutils import sensor_time_to_local, EASTERN, add_survey_time_cols\n",
    "from utils import PHQ_BIWEEKLY_EVENTS_MAIN, PHQ_BIWEEKLY_EVENTS_EXTENDED, MONTHLY_EVENTS_MAIN, MONTHLY_EVENTS_EXTENDED\n",
    "\n",
    "BIWEEKLY_EVENTS = PHQ_BIWEEKLY_EVENTS_MAIN + PHQ_BIWEEKLY_EVENTS_EXTENDED\n",
    "MONTHLY_EVENTS = MONTHLY_EVENTS_MAIN + MONTHLY_EVENTS_EXTENDED\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.filterwarnings('error', category=RuntimeWarning)\n",
    "\n",
    "\n",
    "DATA_FOLDER = \"./data/\"\n",
    "\n",
    "## SENSOR DATA PATHS\n",
    "BLUE_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"bluetooth_fixed_did.csv\")\n",
    "WIFI_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"wifi_fixed_did.csv\")\n",
    "\n",
    "\n",
    "DEVICE_TYPE_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"aware_device.csv\")\n",
    "CALLS_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"calls_fixed_did.csv\")\n",
    "MSGS_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"messages_fixed_did.csv\")\n",
    "\n",
    "LOC_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"locations_semantic_start_date_to_end_date.csv\")\n",
    "SCR_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"screen_clean.csv\")\n",
    "\n",
    "\n",
    "SLP_PATH = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"sleep.csv\")\n",
    "STEPS_PATH = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"steps_total.csv\")\n",
    "STEPS_LEVELS_PATH = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"activity_types_minutes.csv\")\n",
    "HR_PATH = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"hr.csv\")\n",
    "\n",
    "## @TODO - activity levels, \n",
    "## notif, touch, steps intraday, HR\n",
    "\n",
    "## INTRADAY \"FOLDER\" PATHS\n",
    "SLP_INTRADAY_PATH_LIKE = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"sleep_intraday\", \"{0}.csv\")\n",
    "HR_INTRADAY_PATH_LIKE = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"hr_intraday\", \"{0}.csv\")\n",
    "STEPS_INTRADAY_PATH_LIKE = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"steps_intraday\", \"{0}.csv\")\n",
    "VERY_ACT_INTRADAY_PATH_LIKE = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"minutesVeryActive_intraday\", \"{0}.csv\")\n",
    "FAIRLY_ACT_INTRADAY_PATH_LIKE = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"minutesFairlyActive_intraday\", \"{0}.csv\")\n",
    "LIGHTLY_ACT_INTRADAY_PATH_LIKE = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"minutesLightlyActive_intraday\", \"{0}.csv\")\n",
    "SED_ACT_INTRADAY_PATH_LIKE = os.path.join(DATA_FOLDER, \"sensors_fitbit\", \"minutesSedentary_intraday\", \"{0}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "## SURVEY PATH\n",
    "SURVEY_PATH = os.path.join(DATA_FOLDER, \"QTotalScoresMod.csv\")\n",
    "\n",
    "## EMA PATH\n",
    "# EMA_PATH = os.path.join(DATA_FOLDER, \"sensors_clean\", \"survey_clean.csv\")\n",
    "\n",
    "## MAPPING FILE\n",
    "PRT_TO_DID_MAPPING_FILE = os.path.join(DATA_FOLDER, \"mapping_ids/participant_id_MAP_fitbit_id_MAP_device_id.csv\")\n",
    "\n",
    "\n",
    "## OUTPUT SETTINGS\n",
    "OUTPUT_FEATS = True\n",
    "OUTPUT_FEATS_FOLDER = os.path.join(DATA_FOLDER, \"feats_for_multimodal_time_slices\")\n",
    "def output_features(df_feats, sensor_name, feat_type):\n",
    "    df_feats.to_csv(os.path.join(OUTPUT_FEATS_FOLDER, \"feats_{0}_{1}.csv\".format(sensor_name, feat_type)))\n",
    "\n",
    "\n",
    "# ## NORM FEATURES FOR EACH PERSON - ignore for now. may make more sense to norm on per day feats\n",
    "# TO_NORM_PER_PERSON = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ce2fc",
   "metadata": {},
   "source": [
    "# MUST READ\n",
    "## While reading, pay special attention to:\n",
    "### - Consider, when features = nan, can we replace with 0?\n",
    "### - For phase, ensure that all features are from is_during_study (consider this for other scenarios too)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e715142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## device_id to device type\n",
    "DID_TO_TYPE = pd.read_csv(DEVICE_TYPE_PATH)\n",
    "DID_TO_TYPE = DID_TO_TYPE.set_index(\"device_id\")[\"brand\"].to_dict()\n",
    "# display(DID_TO_TYPE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec3642d",
   "metadata": {},
   "source": [
    "# Read and pre-process survey and ema data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38c3b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "## GET SURVEY DATA AND MAP TO DEVICE_ID, ADD EXTRA COLS\n",
    "MAPPING_DF = pd.read_csv(PRT_TO_DID_MAPPING_FILE)\n",
    "MAPPING_DICT = MAPPING_DF.set_index(\"participant_id\")[\"final_device_id\"].to_dict()\n",
    "FB_MAPPING_DICT = MAPPING_DF.set_index(\"fitbit_id\")[\"final_device_id\"].to_dict()\n",
    "FB_MAPPING_DICT_INV = MAPPING_DF.set_index(\"final_device_id\")[\"fitbit_id\"].to_dict()\n",
    "\n",
    "df_survey = pd.read_csv(SURVEY_PATH)\n",
    "df_survey[\"device_id\"] = df_survey[\"record_id\"].apply((lambda x: MAPPING_DICT[x]))\n",
    "df_survey = add_survey_time_cols(df_survey)\n",
    "df_survey = df_survey.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "DIDS_SEL = list(df_survey[\"device_id\"].unique())\n",
    "print (len(DIDS_SEL))\n",
    "\n",
    "# ## GET CLEAN EMA DATA, ADD EXTRA COLS\n",
    "# df_ema = pd.read_csv(EMA_PATH)\n",
    "# df_ema[\"timestamp_dt\"] = df_ema[\"timestamp\"].apply(sensor_time_to_local)\n",
    "# df_ema[\"date_dt\"] = df_ema[\"timestamp_dt\"].dt.date\n",
    "# df_ema = df_ema.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "# EMA_DIDS_SEL = list(df_ema[\"device_id\"].unique())\n",
    "# print (len(EMA_DIDS_SEL))\n",
    "\n",
    "# ## limit dates\n",
    "# df_ema = cut_dates(df_ema, \"timestamp_dt\")\n",
    "\n",
    "# ## NORM \n",
    "# if TO_NORM_PER_PERSON:\n",
    "#     df_ema = norm_per_person_wrapper(df_ema, [\"depression\", \"tiredness\"], \"device_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de99684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>record_id</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>complete_status</th>\n",
       "      <th>age</th>\n",
       "      <th>doe</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>survey_complete</th>\n",
       "      <th>phq9_score</th>\n",
       "      <th>phq12_functioning</th>\n",
       "      <th>msrsr_m</th>\n",
       "      <th>pss_m</th>\n",
       "      <th>mfis_m</th>\n",
       "      <th>psqi_total</th>\n",
       "      <th>pdds</th>\n",
       "      <th>device_id</th>\n",
       "      <th>timestamp_dt</th>\n",
       "      <th>date_dt</th>\n",
       "      <th>timestamp_dt_dow</th>\n",
       "      <th>timestamp_dt_dow_wrt_sat</th>\n",
       "      <th>date_dt_prev_sat</th>\n",
       "      <th>date_dt_win_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>461</td>\n",
       "      <td>PRT180756</td>\n",
       "      <td>week_0</td>\n",
       "      <td>Completed</td>\n",
       "      <td>58</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>2019-11-17 10:09:46</td>\n",
       "      <td>Complete</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0017eb47-eecc-4737-9582-9749dba7f48e</td>\n",
       "      <td>2019-11-17 10:09:46</td>\n",
       "      <td>2019-11-17</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-16</td>\n",
       "      <td>2019-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>462</td>\n",
       "      <td>PRT180756</td>\n",
       "      <td>week_2</td>\n",
       "      <td>Completed</td>\n",
       "      <td>58</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>2019-11-30 12:07:38</td>\n",
       "      <td>Complete</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0017eb47-eecc-4737-9582-9749dba7f48e</td>\n",
       "      <td>2019-11-30 12:07:38</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>2019-11-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  record_id redcap_event_name complete_status  age         doe  \\\n",
       "460         461  PRT180756            week_0       Completed   58  2019-11-12   \n",
       "461         462  PRT180756            week_2       Completed   58  2019-11-12   \n",
       "\n",
       "               timestamp survey_complete  phq9_score  phq12_functioning  \\\n",
       "460  2019-11-17 10:09:46        Complete         0.0                  0   \n",
       "461  2019-11-30 12:07:38        Complete         0.0                  0   \n",
       "\n",
       "     msrsr_m  pss_m  mfis_m  psqi_total  pdds  \\\n",
       "460      7.0   24.0     7.0        10.0   3.0   \n",
       "461      NaN    NaN     NaN         NaN   NaN   \n",
       "\n",
       "                                device_id        timestamp_dt     date_dt  \\\n",
       "460  0017eb47-eecc-4737-9582-9749dba7f48e 2019-11-17 10:09:46  2019-11-17   \n",
       "461  0017eb47-eecc-4737-9582-9749dba7f48e 2019-11-30 12:07:38  2019-11-30   \n",
       "\n",
       "     timestamp_dt_dow  timestamp_dt_dow_wrt_sat date_dt_prev_sat  \\\n",
       "460                 6                         1       2019-11-16   \n",
       "461                 5                         0       2019-11-30   \n",
       "\n",
       "    date_dt_win_start  \n",
       "460        2019-11-03  \n",
       "461        2019-11-16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'record_id', 'redcap_event_name', 'complete_status',\n",
      "       'age', 'doe', 'timestamp', 'survey_complete', 'phq9_score',\n",
      "       'phq12_functioning', 'msrsr_m', 'pss_m', 'mfis_m', 'psqi_total', 'pdds',\n",
      "       'device_id', 'timestamp_dt', 'date_dt', 'timestamp_dt_dow',\n",
      "       'timestamp_dt_dow_wrt_sat', 'date_dt_prev_sat', 'date_dt_win_start'],\n",
      "      dtype='object')\n",
      "2019-11-16 09:08:18\n",
      "2021-01-24 08:48:28\n"
     ]
    }
   ],
   "source": [
    "display(df_survey.head(2))\n",
    "# display(df_ema.head(2))\n",
    "# display(df_survey[\"device_id\"].unique())\n",
    "\n",
    "print (df_survey.columns)\n",
    "\n",
    "\n",
    "# tmp = df_ema.sort_values(by=\"timestamp\", ascending=False)\n",
    "# display(tmp)\n",
    "\n",
    "\n",
    "print (df_survey[\"timestamp_dt\"].min())\n",
    "\n",
    "print (df_survey[\"timestamp_dt\"].max())\n",
    "\n",
    "# print (df_ema[\"timestamp_dt\"].min())\n",
    "\n",
    "# print (df_ema[\"timestamp_dt\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb179c82",
   "metadata": {},
   "source": [
    "# Add a phase and week label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0daee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_phase_label(df, ts_col):\n",
    "    df[\"phase\"] = np.where(df[ts_col]<COVID_CUT_OFF, \"pre\", \\\n",
    "                           np.where( (df[ts_col]>=COVID_CUT_OFF) & (df[ts_col]<LOCKDOWN_YELLOW_START), \"lock\", \\\n",
    "                                    np.where( (df[ts_col]>=LOCKDOWN_YELLOW_START) & (df[ts_col]<LOCKDOWN_GREEN_START), \"yellow\", \\\n",
    "                                             \"green\"\n",
    "                                            )\n",
    "                                   )\n",
    "                          )\n",
    "    return df\n",
    "\n",
    "def add_phase_label_wrapper(df, ts_col):\n",
    "    df = add_phase_label(df, ts_col)\n",
    "    df[\"phase_wrapper\"] = np.where( ((df[\"phase\"]==\"yellow\") |  (df[\"phase\"]==\"green\") ), \"post\", df[\"phase\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# def add_week_label_row_of_person(df_survey_did, data_row, ts_col):\n",
    "# #     display (data_row)\n",
    "# #     display (df_survey_did)\n",
    "#     data_ts = data_row[ts_col]\n",
    "# #     display ((df_survey_did[ts_col]-data_ts).dt.days)\n",
    "# #     print (type((df_survey_did[ts_col]-data_ts)))\n",
    "#     df_survey_gte = df_survey_did[(df_survey_did[ts_col]>=data_ts)]\n",
    "#     df_survey_gte_biweekly = df_survey_gte[(df_survey_gte[\"redcap_event_name\"].isin(BIWEEKLY_EVENTS))]\n",
    "#     df_survey_gte_monthly = df_survey_gte[(df_survey_gte[\"redcap_event_name\"].isin(MONTHLY_EVENTS))]\n",
    "#     df_survey_lte = df_survey_did[(df_survey_did[ts_col]< data_ts)] # is it in between the study\n",
    "# #     display (df_survey_gte)\n",
    "\n",
    "#     ## BIWEEKLY\n",
    "#     if len(df_survey_gte_biweekly)==0: # after the last week\n",
    "#         biweekly_lbl = \"extra\"\n",
    "#         biweekly_date = np.nan\n",
    "#     else:\n",
    "#         df_survey_select_14_days = df_survey_gte_biweekly[((df_survey_gte_biweekly[ts_col]-data_ts).dt.days)<=14]\n",
    "#         if len(df_survey_select_14_days)==0: ## there is a survey after data point but diff is > 14 days\n",
    "#             if len(df_survey_lte)==0: # if there is no survey before data then \"pre-study\"\n",
    "#                 biweekly_lbl = \"pre\"\n",
    "#             else:\n",
    "#                 biweekly_lbl = np.nan # else, missing in between due to delay etc \n",
    "#             biweekly_date = np.nan\n",
    "#         else:\n",
    "#             biweekly_lbl = df_survey_select_14_days[\"redcap_event_name\"].iloc[0]\n",
    "#             biweekly_date = df_survey_select_14_days[\"date_dt\"].iloc[0]\n",
    "\n",
    "#     ## MONTHLY\n",
    "#     if len(df_survey_gte_monthly)==0: # after the last week\n",
    "#         monthly_lbl = \"extra\"\n",
    "#         monthly_date = np.nan\n",
    "#     else:\n",
    "#         df_survey_select_31_days = df_survey_gte_monthly[((df_survey_gte_monthly[ts_col]-data_ts).dt.days)<=31]\n",
    "#         if len(df_survey_select_31_days)==0: ## there is a survey after data point but diff is > 14 days\n",
    "#             if len(df_survey_lte)==0:\n",
    "#                 monthly_lbl = \"pre\"\n",
    "#             else:\n",
    "#                 monthly_lbl = \"np.nan\"\n",
    "#             monthly_date = np.nan\n",
    "#         else:\n",
    "#             monthly_lbl = df_survey_select_31_days[\"redcap_event_name\"].iloc[0]\n",
    "#             monthly_date = df_survey_select_31_days[\"date_dt\"].iloc[0]\n",
    "#     data_row[\"biweekly_lbl\"] = biweekly_lbl\n",
    "#     data_row[\"monthly_lbl\"] = monthly_lbl\n",
    "#     data_row[\"biweekly_date\"] = biweekly_date\n",
    "#     data_row[\"monthly_date\"] = monthly_date\n",
    "#     return data_row\n",
    "            \n",
    "    \n",
    "\n",
    "# def add_week_label(df, ts_col, df_survey):\n",
    "# #     print (type(df[ts_col].iloc[0]))\n",
    "# #     print (type(df_survey[ts_col].iloc[0]))\n",
    "    \n",
    "# #     display(df_survey.head(1))\n",
    "    \n",
    "#     df_survey = df_survey.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "#     df_did_list = []\n",
    "#     print (\"Processing did counts...\")\n",
    "#     cnt = 0\n",
    "#     for did in df_survey[\"device_id\"].unique():\n",
    "#         cnt += 1\n",
    "# #         if cnt != 6: # debugging\n",
    "# #             continue\n",
    "#         print (\"{0}, \".format(cnt), end='')\n",
    "#         df_survey_did = df_survey[(df_survey[\"device_id\"]==did)][[ts_col, \"redcap_event_name\"]]\n",
    "#         df_did = df[(df[\"device_id\"]==did)]\n",
    "#         if len(df_did)==0:\n",
    "#             continue\n",
    "#         df_did = df_did.apply((lambda x: (add_week_label_row_of_person(df_survey_did, x, ts_col))), axis=1)\n",
    "#         df_did[\"biweekly_date_dt_diff\"] = df_did[\"biweekly_date\"] - df_did[\"date_dt\"] \n",
    "#         df_did[\"monthly_date_dt_diff\"] = df_did[\"monthly_date\"] - df_did[\"date_dt\"] \n",
    "#         cols_disp = [\"timestamp_dt\", \"date_dt\", \"biweekly_lbl\", \"monthly_lbl\", \\\n",
    "#                         \"biweekly_date\", \"monthly_date\",\\\n",
    "#                         \"biweekly_date_dt_diff\", \"monthly_date_dt_diff\"\n",
    "#                        ]\n",
    "# #         display(df_did[cols_disp])\n",
    "# #         display(df_did)\n",
    "#         df_did_list.append(df_did)\n",
    "# #         break # debug\n",
    "#     df_dids_all = pd.concat(df_did_list)\n",
    "#     return df_dids_all\n",
    "\n",
    "def add_week_label(df, ts_col, df_survey, print_out=True):\n",
    "    df_survey = df_survey.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "    df_did_list = []\n",
    "    if print_out:\n",
    "        print (\"Processing did counts (tweak)...\")\n",
    "    cnt = 0\n",
    "    for did in df_survey[\"device_id\"].unique():\n",
    "        cnt += 1\n",
    "#         if cnt > 1: # debugging\n",
    "#             break\n",
    "        if print_out:\n",
    "            print (\"{0}, \".format(cnt), end='')\n",
    "        df_survey_did = df_survey[(df_survey[\"device_id\"]==did)][[ts_col, \"redcap_event_name\"]]\n",
    "        df_survey_did_biweekly = df_survey_did[(df_survey_did[\"redcap_event_name\"].isin(BIWEEKLY_EVENTS))]\n",
    "        df_survey_did_monthly = df_survey_did[(df_survey_did[\"redcap_event_name\"].isin(MONTHLY_EVENTS))]\n",
    "        df_did = df[(df[\"device_id\"]==did)]\n",
    "        if len(df_did)==0:\n",
    "            continue\n",
    "        df_did[\"biweekly_lbl\"] = None\n",
    "        df_did[\"monthly_lbl\"] = None\n",
    "        df_did[\"biweekly_date\"] = None\n",
    "        df_did[\"monthly_date\"] = None\n",
    "        ## iterate through survey df rows\n",
    "        for index, row in df_survey_did_biweekly.iterrows():\n",
    "            curr_lbl = row[\"redcap_event_name\"]\n",
    "            curr_dt = row[\"date_dt\"]\n",
    "            curr_dt_minus_14 = curr_dt - datetime.timedelta(days=14)\n",
    "#             print (\"{2}: {0}, {1}\".format(curr_dt, curr_dt_minus_14, curr_lbl))\n",
    "            df_did[\"biweekly_lbl\"] = np.where( ( (df_did[\"biweekly_lbl\"].isnull()) & (df_did[\"date_dt\"]<curr_dt) & \\\n",
    "                                                (df_did[\"date_dt\"]>=curr_dt_minus_14)), \\\n",
    "                                              curr_lbl, df_did[\"biweekly_lbl\"])\n",
    "            df_did[\"biweekly_date\"] = np.where( ( (df_did[\"biweekly_date\"].isnull()) & (df_did[\"date_dt\"]<curr_dt) & (df_did[\"date_dt\"]>=curr_dt_minus_14)), curr_dt, df_did[\"biweekly_date\"])\n",
    "        for index, row in df_survey_did_monthly.iterrows():\n",
    "            curr_lbl = row[\"redcap_event_name\"]\n",
    "            curr_dt = row[\"date_dt\"]\n",
    "            curr_dt_minus_31 = curr_dt - datetime.timedelta(days=31) # have to subtract 15 days to include 30\n",
    "#             print (\"{2}: {0}, {1}\".format(curr_dt, curr_dt_minus_31, curr_lbl))\n",
    "            df_did[\"monthly_lbl\"] = np.where( ( (df_did[\"monthly_lbl\"].isnull()) & (df_did[\"date_dt\"]<curr_dt) & (df_did[\"date_dt\"]>=curr_dt_minus_31)), curr_lbl, df_did[\"monthly_lbl\"])\n",
    "            df_did[\"monthly_date\"] = np.where( ( (df_did[\"monthly_date\"].isnull()) & (df_did[\"date_dt\"]<curr_dt) & (df_did[\"date_dt\"]>=curr_dt_minus_31)), curr_dt, df_did[\"monthly_date\"])\n",
    "\n",
    "#         cols_disp = [\"timestamp_dt\", \"date_dt\", \"biweekly_lbl\", \"monthly_lbl\", \\\n",
    "#                         \"biweekly_date\", \"monthly_date\",\\\n",
    "#                        ]\n",
    "#         display(df_did[cols_disp])\n",
    "        df_did_list.append(df_did)\n",
    "# #         break # debug\n",
    "    df_dids_all = pd.concat(df_did_list)\n",
    "    return df_dids_all\n",
    "        \n",
    "    \n",
    "def add_dow_label(df, ts_col=\"timestamp_dt\"):\n",
    "    df[\"dow\"] = df[ts_col].dt.dayofweek\n",
    "    df[\"dow_lbl\"] = np.where(df[\"dow\"].isin([5, 6]), \"wkend\", \"wkdy\")\n",
    "    return df\n",
    "\n",
    "def add_tod_label(df, ts_col=\"timestamp_dt\"):\n",
    "    df[\"hour_dt\"] = df[ts_col].dt.hour\n",
    "    df[\"tod_lbl\"] = np.where(df[\"hour_dt\"].isin([0, 1, 2, 3, 4, 5]), \"ni\", \\\n",
    "                             np.where(df[\"hour_dt\"].isin([6, 7, 8, 9, 10, 11]), \"mo\", \\\n",
    "                                      np.where(df[\"hour_dt\"].isin([12, 13, 14, 15, 16, 17]), \"af\", \"ev\"\n",
    "                                              )\n",
    "                                     )\n",
    "                            )\n",
    "    return df\n",
    "\n",
    "def add_wknum_wrt_wk12(df,  ts_col, df_survey, print_out=True):\n",
    "    df_survey = df_survey.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "    df_did_list = []\n",
    "    if print_out:\n",
    "        print (\"Processing did counts...\")\n",
    "    cnt = 0\n",
    "    for did in df_survey[\"device_id\"].unique():\n",
    "        cnt += 1\n",
    "#         if cnt > 1: # debugging\n",
    "#             break\n",
    "#         print (\"{0}, \".format(cnt), end='')\n",
    "        df_survey_did = df_survey[(df_survey[\"device_id\"]==did)][[ts_col, \"redcap_event_name\"]]\n",
    "        df_did = df[(df[\"device_id\"]==did)]\n",
    "        df_did[\"wknum_wrt_wk12\"] = None\n",
    "        df_survey_did_week_12 = df_survey_did[(df_survey_did[\"redcap_event_name\"]==\"week_12\")]\n",
    "        if len(df_survey_did_week_12)==0:\n",
    "            continue\n",
    "#         display(df_survey_did)\n",
    "        week_12_date = df_survey_did_week_12[\"date_dt\"].iloc[0]\n",
    "        curr_end_date = week_12_date\n",
    "        for i in range(12, 0, -1):\n",
    "            curr_start_date = curr_end_date - datetime.timedelta(days=7)\n",
    "#             print (\"wk {0}: {1} to {2}\".format(i, curr_start_date, curr_end_date))\n",
    "            df_did[\"wknum_wrt_wk12\"] = np.where( ( (df_did[\"date_dt\"]>=curr_start_date) & (df_did[\"date_dt\"]<curr_end_date) ), \"wknum_{0}\".format(i), df_did[\"wknum_wrt_wk12\"])\n",
    "            curr_end_date = curr_start_date\n",
    "#         display(df_did)\n",
    "#         break # debug\n",
    "        df_did_list.append(df_did)\n",
    "    df_dids_all = pd.concat(df_did_list)\n",
    "    return df_dids_all\n",
    "\n",
    "\n",
    "def get_during_study_lbl(df, df_survey, print_out=True):\n",
    "    ''' Generates a label that can be used to exclude samples before week_0 and after the last week\n",
    "    Includes the days week_0 and week_last surveys were taken\n",
    "    '''\n",
    "    df_survey = df_survey.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "    df_did_list = []\n",
    "    if print_out:\n",
    "        print (\"Processing did counts...\")\n",
    "    cnt = 0\n",
    "    for did in df_survey[\"device_id\"].unique():\n",
    "        cnt += 1\n",
    "#         if cnt > 1: # debugging\n",
    "#             break\n",
    "#         print (\"{0}, \".format(cnt), end='')\n",
    "        df_survey_did = df_survey[(df_survey[\"device_id\"]==did)][[\"date_dt\", \"redcap_event_name\"]]\n",
    "        df_did = df[(df[\"device_id\"]==did)]\n",
    "        df_did[\"is_during_study\"] = None\n",
    "        df_survey_did_week_0 = df_survey_did[\"date_dt\"].iloc[0]\n",
    "        df_survey_did_week_last = df_survey_did[\"date_dt\"].iloc[-1]\n",
    "#         print (\"{0}: {1} to {2}\".format(did, df_survey_did_week_0, df_survey_did_week_last))\n",
    "        df_did[\"is_during_study\"] = np.where( ( (df_did[\"date_dt\"]>=df_survey_did_week_0) & (df_did[\"date_dt\"]<=df_survey_did_week_last) ), True, df_did[\"is_during_study\"])\n",
    "        df_did_list.append(df_did)\n",
    "#         display(df_did.head(100))\n",
    "#         display(df_did.tail(100))\n",
    "    df_dids_all = pd.concat(df_did_list)\n",
    "    return df_dids_all\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def add_all_epoch_labels(df, df_survey, print_out=True):\n",
    "    ## Add PHASE\n",
    "    df =  add_phase_label_wrapper(df, \"timestamp_dt\")  \n",
    "    # Add week label\n",
    "    df =  add_week_label(df, \"date_dt\", df_survey, print_out)  \n",
    "    # add dow label\n",
    "    df =  add_dow_label(df, \"timestamp_dt\")  \n",
    "    # add tod label\n",
    "    df =  add_tod_label(df, \"timestamp_dt\")  \n",
    "    # add week nnum wrt week 12\n",
    "    df = add_wknum_wrt_wk12(df, \"date_dt\", df_survey, print_out)\n",
    "    # add is_during_study\n",
    "    df = get_during_study_lbl(df, df_survey, print_out)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfc861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## GET SURVEY DATA AND MAP TO DEVICE_ID, ADD EXTRA COLS\n",
    "# MAPPING_DF = pd.read_csv(PRT_TO_DID_MAPPING_FILE)\n",
    "# MAPPING_DICT = MAPPING_DF.set_index(\"participant_id\")[\"final_device_id\"].to_dict()\n",
    "# FB_MAPPING_DICT = MAPPING_DF.set_index(\"fitbit_id\")[\"final_device_id\"].to_dict()\n",
    "\n",
    "# df_survey = pd.read_csv(SURVEY_PATH)\n",
    "# df_survey[\"device_id\"] = df_survey[\"record_id\"].apply((lambda x: MAPPING_DICT[x]))\n",
    "# df_survey = add_survey_time_cols(df_survey)\n",
    "# df_survey = df_survey.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_survey =  add_phase_label_wrapper(df_survey, \"timestamp_dt\")  \n",
    "\n",
    "\n",
    "# # ## GET CLEAN EMA DATA, ADD EXTRA COLS\n",
    "# # df_ema = pd.read_csv(EMA_PATH)\n",
    "# # df_ema[\"timestamp_dt\"] = df_ema[\"timestamp\"].apply(sensor_time_to_local)\n",
    "# # df_ema[\"date_dt\"] = df_ema[\"timestamp_dt\"].dt.date\n",
    "# # df_ema = df_ema.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "# # ## NORM \n",
    "# # if TO_NORM_PER_PERSON:\n",
    "# #     df_ema = norm_per_person_wrapper(df_ema, [\"depression\", \"tiredness\"], \"device_id\")\n",
    "\n",
    "\n",
    "\n",
    "# # df_ema_per_day = df_ema.groupby([\"device_id\", \"date_dt\"]).agg(mean_dep=(\"depression\", \"mean\"),\\\n",
    "# #                                                               max_dep=(\"depression\", \"max\"),\\\n",
    "# #                                                               n_dep=(\"depression\", \"count\"),\\\n",
    "# #                                                               mean_tir=(\"tiredness\", \"mean\"),\\\n",
    "# #                                                               max_tir=(\"tiredness\", \"max\"),\\\n",
    "# #                                                               n_tir=(\"tiredness\", \"count\"),\\\n",
    "# #                                                              ).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229f51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_survey.columns)\n",
    "# display(df_survey.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3c88f",
   "metadata": {},
   "source": [
    "# Create feature function for means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6d3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_mean(g, INPUT_FEAT_NAMES):\n",
    "    feats_dict = {}\n",
    "    if len(g)==1:\n",
    "        for feat in INPUT_FEAT_NAMES:\n",
    "            feats_dict[feat] = g[feat].iloc[0]\n",
    "    elif len(g)==0:\n",
    "        for feat in INPUT_FEAT_NAMES:\n",
    "            feats_dict[feat] = np.nan\n",
    "    else:\n",
    "        for feat in INPUT_FEAT_NAMES:\n",
    "            feats_dict[feat] = g[feat].mean()\n",
    "    return pd.Series(feats_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3bf86",
   "metadata": {},
   "source": [
    "# Bluetooth - clean and add phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c2d36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_blue = pd.read_csv(BLUE_PATH)\n",
    "df_blue[\"timestamp_dt\"] = df_blue[\"timestamp\"].apply(sensor_time_to_local)\n",
    "df_blue[\"date_dt\"] = df_blue[\"timestamp_dt\"].dt.date\n",
    "df_blue = df_blue.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "# blue_input_feats = [\"num_scans_of_most_frequent_device\", \\\n",
    "#                     \"num_scans_of_least_frequent_device\", \\\n",
    "#                     \"number_unique_devices\", \\\n",
    "#                     \"num_scans_of_most_frequent_device_of_others\", \\\n",
    "#                     \"num_scans_of_least_frequent_device_of_others\", \\\n",
    "#                     \"number_unique_devices_of_others\", \\\n",
    "#                     \"num_scans_of_most_frequent_device_of_self\", \\\n",
    "#                     \"num_scans_of_least_frequent_device_of_self\", \\\n",
    "#                     \"number_unique_devices_of_self\", \\\n",
    "#                     \"sum_num_scans_of_all_devices_of_self\", \\\n",
    "#                     \"sum_num_scans_of_all_devices_of_others\", \\\n",
    "#                     \"avg_num_scans_of_all_devices_of_self\", \\\n",
    "#                     \"avg_num_scans_of_all_devices_of_others\"]\n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_blue =  add_all_epoch_labels(df_blue, df_survey) \n",
    "df_blue_in_study = df_blue[(df_blue[\"is_during_study\"]==True)] # helps exclude incomplete days\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c151b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(df_blue[df_blue[\"biweekly_lbl\"].isnull()])\n",
    "# display(df_blue_in_study[df_blue_in_study[\"biweekly_lbl\"].isnull()])\n",
    "\n",
    "# display(df_blue.iloc[550:700])\n",
    "# display(df_blue.head(20))\n",
    "display(df_blue_in_study.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display (df_blue.head(2))\n",
    "\n",
    "def get_df_blue_clusters(df_blue):\n",
    "    ## Calculate frequency of each bt address \n",
    "    df_blue_freq = df_blue.groupby([\"device_id\", \"bt_address\"]).count()[[\"date_dt\"]]\n",
    "    df_blue_freq = df_blue_freq.rename(columns={\"date_dt\": \"freq\"})\n",
    "\n",
    "    ## Calculate num days of each bt address \n",
    "    df_blue_numdays = pd.DataFrame(df_blue.groupby([\"device_id\", \"bt_address\"])[\"date_dt\"].nunique())\n",
    "    df_blue_numdays = df_blue_numdays.rename(columns={\"date_dt\": \"numdays\"})\n",
    "\n",
    "    ## Concat\n",
    "    df_blue_avgfreq = df_blue_freq.join(df_blue_numdays)\n",
    "    df_blue_avgfreq[\"avgfreq\"] = df_blue_avgfreq[\"freq\"]/df_blue_avgfreq[\"numdays\"]\n",
    "    df_blue_avgfreq = df_blue_avgfreq.reset_index()\n",
    "    df_blue_avgfreq = df_blue_avgfreq.sort_values(by=[\"device_id\", \"freq\", \"bt_address\"], ascending=[True, False, True])\n",
    "    # display (df_blue_avgfreq.head(20))\n",
    "    ## Clustering for each device_id separately\n",
    "    df_blue_clust_list = []\n",
    "    did_list = []\n",
    "    num_clusters_list = []\n",
    "    own_devices_list_of_list = []\n",
    "    num_own_devices_list = []\n",
    "    did_to_own_devices_list = {}\n",
    "    for did in df_blue_avgfreq[\"device_id\"].unique():\n",
    "        df_blue_avgfreq_did = df_blue_avgfreq[(df_blue_avgfreq[\"device_id\"]==did)]\n",
    "        df_blue_clust_did, numclust_did = cluster_address_freq(df_blue_avgfreq_did)\n",
    "        df_blue_clust_list.append(df_blue_clust_did)\n",
    "    #     display (df_blue_clust_did.head(2))\n",
    "        did_list.append(did)\n",
    "        num_clusters_list.append(numclust_did)\n",
    "        if (numclust_did is None) or (numclust_did == 0):\n",
    "            own_devices_list_of_list.append([])\n",
    "            num_own_devices_list.append(0)\n",
    "            did_to_own_devices_list[did] = []\n",
    "        else:\n",
    "            owndevices_did = getOwnDevices(df_blue_clust_did)\n",
    "            own_devices_list_of_list.append(owndevices_did)  \n",
    "            num_own_devices_list.append(len(owndevices_did))\n",
    "            did_to_own_devices_list[did] = owndevices_did\n",
    "\n",
    "    df_blue_clust = pd.concat(df_blue_clust_list)\n",
    "    df_blue_user_clust = pd.DataFrame({\"device_id\": did_list, \"num_clusters\": num_clusters_list, \\\n",
    "                                       \"own_devices_list\": own_devices_list_of_list, \\\n",
    "                                       \"num_own_devices\": num_own_devices_list\n",
    "                                      })\n",
    "\n",
    "\n",
    "    df_blue_user_clust = df_blue_user_clust.set_index(\"device_id\")\n",
    "    df_blue_user_clust_owndevices_dict = df_blue_user_clust[\"own_devices_list\"]\n",
    "    return (df_blue_clust, df_blue_user_clust, df_blue_user_clust_owndevices_dict, did_to_own_devices_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR CLUSTERING ONLY USE SAMPLES IN_DURING_STUDY (complete days)\n",
    "df_blue_clust, df_blue_user_clust, df_blue_user_clust_owndevices_dict, did_to_own_devices_list = get_df_blue_clusters(df_blue_in_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7345bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display (df_blue_clust.head(2))\n",
    "# display (df_blue_user_clust.head(2))\n",
    "print (len(df_blue_clust))\n",
    "print (len(df_blue_user_clust))\n",
    "\n",
    "display (df_blue_user_clust.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbaa67d",
   "metadata": {},
   "source": [
    "### Bluetooth get per phase/week features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_blue(g, did_to_own_devices_list):\n",
    "    if \"device_id\" in g.columns:\n",
    "        did = g[\"device_id\"].iloc[0]\n",
    "    else:\n",
    "        did = g.name[0]\n",
    "    owndevices = did_to_own_devices_list[did]\n",
    "    feats_dict = {}\n",
    "    feats_dict[\"num_scans_of_most_frequent_device\"] = num_scans_of_most_frequent_device(g)\n",
    "    feats_dict[\"num_scans_of_least_frequent_device\"] = num_scans_of_least_frequent_device(g)\n",
    "    feats_dict[\"number_unique_devices\"] = number_unique_devices(g)\n",
    "    \n",
    "    feats_dict[\"num_scans_of_most_frequent_device_of_others\"] = num_scans_of_most_frequent_device_of_others(g, [owndevices])\n",
    "    feats_dict[\"num_scans_of_least_frequent_device_of_others\"] = num_scans_of_least_frequent_device_of_others(g, [owndevices])\n",
    "    feats_dict[\"number_unique_devices_of_others\"] = number_unique_devices_of_others(g, [owndevices])\n",
    "    \n",
    "    feats_dict[\"num_scans_of_most_frequent_device_of_self\"] = num_scans_of_most_frequent_device_of_self(g, [owndevices])\n",
    "    feats_dict[\"num_scans_of_least_frequent_device_of_self\"] = num_scans_of_least_frequent_device_of_self(g, [owndevices])\n",
    "    feats_dict[\"number_unique_devices_of_self\"] = number_unique_devices_of_self(g, [owndevices])\n",
    "    \n",
    "    feats_dict[\"sum_num_scans_of_all_devices_of_self\"] = sum_num_scans_of_all_devices_of_self(g, [owndevices])\n",
    "    feats_dict[\"sum_num_scans_of_all_devices_of_others\"] = sum_num_scans_of_all_devices_of_others(g, [owndevices])\n",
    "    \n",
    "    feats_dict[\"avg_num_scans_of_all_devices_of_self\"] = avg_num_scans_of_all_devices_of_self(g, [owndevices])\n",
    "    feats_dict[\"avg_num_scans_of_all_devices_of_others\"] = avg_num_scans_of_all_devices_of_others(g, [owndevices])\n",
    "    \n",
    "#     feats_dict[\"std_num_scans_of_all_devices_of_self\"] = std_num_scans_of_all_devices_of_self(g, [owndevices])\n",
    "#     feats_dict[\"std_num_scans_of_all_devices_of_others\"] = std_num_scans_of_all_devices_of_others(g, [owndevices])\n",
    "    ## too many nulls for stds\n",
    "    return pd.Series(feats_dict)\n",
    "\n",
    "\n",
    "def mean_blue_feats_across_days(df_grp, did_to_own_devices_list):\n",
    "    df_grp_feats_per_day = df_grp.groupby(\"date_dt\", as_index=False).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index(drop=True)\n",
    "    df_grp_feats_per_day_mean = df_grp_feats_per_day.mean().to_frame().transpose()\n",
    "#     display(df_grp_feats_per_day_mean)\n",
    "    return df_grp_feats_per_day_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09921858",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##per phase - use only complete days so that mean is not skewed\n",
    "##we are averaging across days as people may have different number of days in each phase\n",
    "df_blue_phase_feats = df_blue_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: mean_blue_feats_across_days(x, did_to_own_devices_list))).reset_index()\n",
    "df_blue_phase_feats = df_blue_phase_feats.drop(columns=[\"level_2\"])\n",
    "output_features(df_blue_phase_feats, \"blue\", \"per_phase\")\n",
    "\n",
    "##per-phase tod\n",
    "df_blue_phase_feats = df_blue_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: mean_blue_feats_across_days(x, did_to_own_devices_list))).reset_index()\n",
    "df_blue_phase_feats = df_blue_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_blue_phase_feats, \"blue\", \"per_phase_tod\")\n",
    "\n",
    "##per-phase dow\n",
    "df_blue_phase_feats = df_blue_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: mean_blue_feats_across_days(x, did_to_own_devices_list))).reset_index()\n",
    "df_blue_phase_feats = df_blue_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_blue_phase_feats, \"blue\", \"per_phase_dow\")\n",
    "\n",
    "##per-phase tod, dow\n",
    "df_blue_phase_feats = df_blue_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: mean_blue_feats_across_days(x, did_to_own_devices_list))).reset_index()\n",
    "df_blue_phase_feats = df_blue_phase_feats.drop(columns=[\"level_4\"])\n",
    "output_features(df_blue_phase_feats, \"blue\", \"per_phase_tod_dow\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # per-phase\n",
    "# df_blue_phase_feats = df_blue.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_phase_feats, \"blue\", \"per_phase\")\n",
    "# # per-phase tod\n",
    "# df_blue_phase_feats = df_blue.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_phase_feats, \"blue\", \"per_phase_tod\")\n",
    "# # per-phase dow\n",
    "# df_blue_phase_feats = df_blue.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_phase_feats, \"blue\", \"per_phase_dow\")\n",
    "# # per-phase tod, dow\n",
    "# df_blue_phase_feats = df_blue.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_phase_feats, \"blue\", \"per_phase_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_blue_phase_feats.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85681389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # biweekly\n",
    "# df_blue_biweekly_feats = df_blue.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_biweekly_feats, \"blue\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_blue_biweekly_feats = df_blue.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_biweekly_feats, \"blue\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_blue_biweekly_feats = df_blue.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_biweekly_feats, \"blue\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow\n",
    "# df_blue_biweekly_feats = df_blue.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_biweekly_feats, \"blue\", \"biweekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # monthly\n",
    "# df_blue_monthly_feats = df_blue.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_monthly_feats, \"blue\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_blue_monthly_feats = df_blue.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_monthly_feats, \"blue\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_blue_monthly_feats = df_blue.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_monthly_feats, \"blue\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_blue_monthly_feats = df_blue.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_monthly_feats, \"blue\", \"monthly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67526d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weekly\n",
    "# df_blue_weekly_feats = df_blue.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_weekly_feats, \"blue\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_blue_weekly_feats = df_blue.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_weekly_feats, \"blue\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_blue_weekly_feats = df_blue.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_weekly_feats, \"blue\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_blue_weekly_feats = df_blue.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_blue(x, did_to_own_devices_list))).reset_index()\n",
    "# output_features(df_blue_weekly_feats, \"blue\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca79d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_blue_biweekly_feats.head(2))\n",
    "# display(df_blue_monthly_feats.head(2))\n",
    "\n",
    "# display(df_blue_biweekly_feats.isnull().sum(axis = 0))\n",
    "# display(df_blue_monthly_feats.isnull().sum(axis = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac032da",
   "metadata": {},
   "source": [
    "# Calls - clean and add phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb79491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls = pd.read_csv(CALLS_PATH)\n",
    "df_calls[\"timestamp_dt\"] = df_calls[\"timestamp\"].apply(sensor_time_to_local)\n",
    "df_calls[\"date_dt\"] = df_calls[\"timestamp_dt\"].dt.date\n",
    "df_calls = df_calls.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f91179",
   "metadata": {},
   "outputs": [],
   "source": [
    "DID_MAPPED = {}\n",
    "def get_device_type(did_old, df_calls_in):\n",
    "    if did_old in DID_MAPPED:\n",
    "        dtype = DID_MAPPED[did_old]\n",
    "    elif did_old in DID_TO_TYPE:\n",
    "        dtype = DID_TO_TYPE[did_old]\n",
    "    else:\n",
    "        df = df_calls_in.copy(deep=True)\n",
    "        df_did_old_any_4 = df[( (df[\"device_id_old\"]==did_old) & (df[\"call_type\"]==4) ) ]\n",
    "        if len(df_did_old_any_4)>0:\n",
    "            dtype = \"iPhone\"\n",
    "        else:\n",
    "            dtype = \"Android\"\n",
    "        print (\"{0} not in aware_device (guess = {1})\".format(did_old, dtype))\n",
    "    DID_MAPPED[did_old] = dtype\n",
    "    return dtype\n",
    "        \n",
    "\n",
    "def reformat_ios_data(g_in):\n",
    "    if g_in is None:\n",
    "        return None\n",
    "    g = g_in[(g_in[\"device_type\"]==\"iPhone\")]\n",
    "    g_android = g_in[(g_in[\"device_type\"]!=\"iPhone\")]\n",
    "    if len(g)>0:\n",
    "        df_incoming = g[(g[\"call_type\"] == 4) & (g[\"call_type\"].shift(1) == 2) & (g[\"call_type\"].shift(2) == 1)]\n",
    "        df_outgoing = g[(g[\"call_type\"] == 4) & (g[\"call_type\"].shift(1) == 2) & (g[\"call_type\"].shift(2) == 3)]\n",
    "        df_missed = g[(g[\"call_type\"] == 4) & ((g[\"call_type\"].shift(1) == 1) | (g[\"call_type\"].shift(1) == 3))]\n",
    "        df_incoming[\"call_type\"] = 1\n",
    "        df_outgoing[\"call_type\"] = 2\n",
    "        df_missed[\"call_type\"] = 3\n",
    "        df = df_incoming.append(df_outgoing)\n",
    "        df = df.append(df_missed)\n",
    "        if len(g_android) > 0:\n",
    "            df = df.append(g_android)\n",
    "    else:\n",
    "        df = g_android\n",
    "    df = df.sort_values(by=['timestamp_dt'])\n",
    "    return df\n",
    "\n",
    "      \n",
    "\n",
    "df_calls[\"device_type\"] = df_calls[\"device_id_old\"].apply((lambda x: get_device_type(x, df_calls)))\n",
    "\n",
    "df_calls = reformat_ios_data(df_calls)\n",
    "\n",
    "\n",
    "# calls_input_feats = [\"number_outgoing_calls\", \"number_incoming_calls\", \"number_missed_calls\", \\\n",
    "#                      \"duration_outgoing_calls_seconds\", \"duration_incoming_calls_seconds\", \\\n",
    "#                      \"number_of_correspondents_phone\"]\n",
    "\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_calls =  add_phase_label_wrapper(df_calls, \"timestamp_dt\")  \n",
    "\n",
    "# # Add week label\n",
    "# df_calls =  add_week_label(df_calls, \"date_dt\", df_survey)  \n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_calls =  add_all_epoch_labels(df_calls, df_survey)  \n",
    "df_calls_in_study = df_calls[(df_calls[\"is_during_study\"]==True)] # helps exclude incomplete days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_calls.head(2))\n",
    "\n",
    "# tmp = df_calls[(df_calls[\"call_type\"]==4)]\n",
    "# print (len(tmp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9612703",
   "metadata": {},
   "source": [
    "### Calls - feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63073be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_calls(g):\n",
    "    feats_dict = {}\n",
    "    feats_dict[\"number_outgoing_calls\"] = number_outgoing_calls(g)\n",
    "    feats_dict[\"number_incoming_calls\"] = number_incoming_calls(g)\n",
    "    feats_dict[\"number_missed_calls\"] = number_missed_calls(g)\n",
    "    feats_dict[\"duration_outgoing_calls_seconds\"] = duration_outgoing_calls_seconds(g)\n",
    "    feats_dict[\"duration_incoming_calls_seconds\"] = duration_incoming_calls_seconds(g)\n",
    "    feats_dict[\"number_of_correspondents_phone\"] = number_of_correspondents_phone(g)\n",
    "    return pd.Series(feats_dict)\n",
    "\n",
    "def mean_calls_feats_across_days(df_grp):\n",
    "    df_grp_feats_per_day = df_grp.groupby(\"date_dt\", as_index=False).apply((lambda x: add_features_calls(x))).reset_index(drop=True)\n",
    "    df_grp_feats_per_day_mean = df_grp_feats_per_day.mean().to_frame().transpose()\n",
    "#     display(df_grp_feats_per_day_mean)\n",
    "    return df_grp_feats_per_day_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e375bb",
   "metadata": {},
   "source": [
    "### Calls - Get per-phase/week features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db331cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##per phase - use only complete days so that mean is not skewed\n",
    "##we are averaging across days as people may have different number of days in each phase\n",
    "df_calls_phase_feats = df_calls_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: mean_calls_feats_across_days(x))).reset_index()\n",
    "df_calls_phase_feats = df_calls_phase_feats.drop(columns=[\"level_2\"])\n",
    "output_features(df_calls_phase_feats, \"calls\", \"per_phase\")\n",
    "\n",
    "##per-phase tod\n",
    "df_calls_phase_feats = df_calls_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: mean_calls_feats_across_days(x))).reset_index()\n",
    "df_calls_phase_feats = df_calls_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_calls_phase_feats, \"calls\", \"per_phase_tod\")\n",
    "\n",
    "##per-phase dow\n",
    "df_calls_phase_feats = df_calls_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: mean_calls_feats_across_days(x))).reset_index()\n",
    "df_calls_phase_feats = df_calls_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_calls_phase_feats, \"calls\", \"per_phase_dow\")\n",
    "\n",
    "##per-phase tod, dow\n",
    "df_calls_phase_feats = df_calls_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: mean_calls_feats_across_days(x))).reset_index()\n",
    "df_calls_phase_feats = df_calls_phase_feats.drop(columns=[\"level_4\"])\n",
    "output_features(df_calls_phase_feats, \"calls\", \"per_phase_tod_dow\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # per-phase\n",
    "# df_calls_phase_feats = df_calls.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_phase_feats, \"calls\", \"per_phase\")\n",
    "# # per-phase tod\n",
    "# df_calls_phase_feats = df_calls.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_phase_feats, \"calls\", \"per_phase_tod\")\n",
    "# # per-phase dow\n",
    "# df_calls_phase_feats = df_calls.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_phase_feats, \"calls\", \"per_phase_dow\")\n",
    "# # per-phase tod, dow\n",
    "# df_calls_phase_feats = df_calls.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_phase_feats, \"calls\", \"per_phase_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_calls_phase_feats.head(2))\n",
    "# display(df_calls_phase_feats.isnull().sum(axis = 0))\n",
    "\n",
    "\n",
    "# # biweekly\n",
    "# df_calls_biweekly_feats = df_calls.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_biweekly_feats, \"calls\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_calls_biweekly_feats = df_calls.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_biweekly_feats, \"calls\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_calls_biweekly_feats = df_calls.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_biweekly_feats, \"calls\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow\n",
    "# df_calls_biweekly_feats = df_calls.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_biweekly_feats, \"calls\", \"biweekly_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ff903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_calls_biweekly_feats = df_calls.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# df_calls_monthly_feats = df_calls.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "\n",
    "# # monthly\n",
    "# df_calls_monthly_feats = df_calls.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_monthly_feats, \"calls\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_calls_monthly_feats = df_calls.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_monthly_feats, \"calls\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_calls_monthly_feats = df_calls.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_monthly_feats, \"calls\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_calls_monthly_feats = df_calls.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_monthly_feats, \"calls\", \"monthly_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weekly\n",
    "# df_calls_weekly_feats = df_calls.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_weekly_feats, \"calls\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_calls_weekly_feats = df_calls.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_weekly_feats, \"calls\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_calls_weekly_feats = df_calls.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_weekly_feats, \"calls\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_calls_weekly_feats = df_calls.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_calls(x))).reset_index()\n",
    "# output_features(df_calls_weekly_feats, \"calls\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c74ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_calls_biweekly_feats.head(2))\n",
    "# display(df_calls_biweekly_feats.isnull().sum(axis = 0))\n",
    "# display(df_calls_monthly_feats.head(2))\n",
    "# display(df_calls_monthly_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# ## OUTPUT\n",
    "# output_features(df_calls_biweekly_feats, \"calls\", \"biweekly\")\n",
    "# output_features(df_calls_monthly_feats, \"calls\", \"monthly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e582bbe",
   "metadata": {},
   "source": [
    "# Msgs - clean and add phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa060f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msgs = pd.read_csv(MSGS_PATH)\n",
    "df_msgs[\"timestamp_dt\"] = df_msgs[\"timestamp\"].apply(sensor_time_to_local)\n",
    "df_msgs[\"date_dt\"] = df_msgs[\"timestamp_dt\"].dt.date\n",
    "df_msgs = df_msgs.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "# msgs_input_feats = [\"number_of_outgoing_messages\", \\\n",
    "#                     \"number_of_incoming_messages\", \\\n",
    "#                     \"number_of_correspondents\"\n",
    "#                    ]\n",
    "\n",
    "\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_msgs =  add_phase_label_wrapper(df_msgs, \"timestamp_dt\")  \n",
    "\n",
    "# # Add week label\n",
    "# df_msgs =  add_week_label(df_msgs, \"date_dt\", df_survey)\n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_msgs =  add_all_epoch_labels(df_msgs, df_survey)  \n",
    "df_msgs_in_study = df_msgs[(df_msgs[\"is_during_study\"]==True)] # helps exclude incomplete days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_msgs.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d31e0a",
   "metadata": {},
   "source": [
    "### Messages - feature function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_msgs(g):\n",
    "    feats_dict = {}\n",
    "    feats_dict[\"number_of_outgoing_messages\"] = number_of_outgoing_messages(g)\n",
    "    feats_dict[\"number_of_incoming_messages\"] = number_of_incoming_messages(g)\n",
    "    feats_dict[\"number_of_correspondents\"] = number_of_correspondents(g)\n",
    "    return pd.Series(feats_dict)\n",
    "\n",
    "def mean_msgs_feats_across_days(df_grp):\n",
    "    df_grp_feats_per_day = df_grp.groupby(\"date_dt\", as_index=False).apply((lambda x: add_features_msgs(x))).reset_index(drop=True)\n",
    "    df_grp_feats_per_day_mean = df_grp_feats_per_day.mean().to_frame().transpose()\n",
    "#     display(df_grp_feats_per_day_mean)\n",
    "    return df_grp_feats_per_day_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca02ab",
   "metadata": {},
   "source": [
    "### Msgs - Get per-phase/week features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db68549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per phase - use only complete days so that mean is not skewed\n",
    "# we are averaging across days as people may have different number of days in each phase\n",
    "# df_msgs_phase_feats = df_msgs_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: mean_blue_msgs_across_days(x))).reset_index()\n",
    "# df_msgs_phase_feats = df_msgs_phase_feats.drop(columns=[\"level_2\"])\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase\")\n",
    "\n",
    "# per-phase tod\n",
    "# df_msgs_phase_feats = df_msgs_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: mean_msgs_feats_across_days(x))).reset_index()\n",
    "# df_msgs_phase_feats = df_msgs_phase_feats.drop(columns=[\"level_3\"])\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase_tod\")\n",
    "\n",
    "# per-phase dow\n",
    "# df_msgs_phase_feats = df_msgs_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: mean_msgs_feats_across_days(x))).reset_index()\n",
    "# df_msgs_phase_feats = df_msgs_phase_feats.drop(columns=[\"level_3\"])\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase_dow\")\n",
    "\n",
    "# per-phase tod, dow\n",
    "# df_msgs_phase_feats = df_msgs_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: mean_msgs_feats_across_days(x))).reset_index()\n",
    "# df_msgs_phase_feats = df_msgs_phase_feats.drop(columns=[\"level_4\"])\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase_tod_dow\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # per-phase\n",
    "# df_msgs_phase_feats = df_msgs.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase\")\n",
    "# # per-phase tod\n",
    "# df_msgs_phase_feats = df_msgs.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase_tod\")\n",
    "# # per-phase dow\n",
    "# df_msgs_phase_feats = df_msgs.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase_dow\")\n",
    "# # per-phase tod, dow\n",
    "# df_msgs_phase_feats = df_msgs.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_phase_feats, \"msgs\", \"per_phase_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_msgs_phase_feats.head(2))\n",
    "# display(df_msgs_phase_feats.isnull().sum(axis = 0))\n",
    "# df_msgs_biweekly_feats = df_msgs.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# df_msgs_monthly_feats = df_msgs.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "\n",
    "# # biweekly\n",
    "# df_msgs_biweekly_feats = df_msgs.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_biweekly_feats, \"msgs\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_msgs_biweekly_feats = df_msgs.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_biweekly_feats, \"msgs\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_msgs_biweekly_feats = df_msgs.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_biweekly_feats, \"msgs\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow\n",
    "# df_msgs_biweekly_feats = df_msgs.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_biweekly_feats, \"msgs\", \"biweekly_tod_dow\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7599b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # monthly\n",
    "# df_msgs_monthly_feats = df_msgs.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_monthly_feats, \"msgs\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_msgs_monthly_feats = df_msgs.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_monthly_feats, \"msgs\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_msgs_monthly_feats = df_msgs.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_monthly_feats, \"msgs\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_msgs_monthly_feats = df_msgs.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_monthly_feats, \"msgs\", \"monthly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2612b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weekly\n",
    "# df_msgs_weekly_feats = df_msgs.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_weekly_feats, \"msgs\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_msgs_weekly_feats = df_msgs.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_weekly_feats, \"msgs\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_msgs_weekly_feats = df_msgs.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_weekly_feats, \"msgs\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_msgs_weekly_feats = df_msgs.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_msgs(x))).reset_index()\n",
    "# output_features(df_msgs_weekly_feats, \"msgs\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c60a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_msgs_biweekly_feats.head(2))\n",
    "# display(df_msgs_biweekly_feats.isnull().sum(axis = 0))\n",
    "# display(df_msgs_monthly_feats.head(2))\n",
    "# display(df_msgs_monthly_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# ## OUTPUT\n",
    "# output_features(df_msgs_biweekly_feats, \"msgs\", \"biweekly\")\n",
    "# output_features(df_msgs_monthly_feats, \"msgs\", \"monthly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb154504",
   "metadata": {},
   "source": [
    "# Sleep - Clean and add phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slp = pd.read_csv(SLP_PATH)\n",
    "df_slp[\"timestamp_dt\"] = pd.to_datetime(df_slp[\"date\"], format=\"%Y-%m-%d\")\n",
    "df_slp[\"date_dt\"] = df_slp[\"timestamp_dt\"].dt.date\n",
    "df_slp[\"device_id\"] = df_slp[\"fitbit_id\"].apply(lambda x: FB_MAPPING_DICT[x])\n",
    "df_slp = df_slp.sort_values(by=[\"device_id\", \"date_dt\"])\n",
    "\n",
    "slp_input_feats = ['totalMinutesAsleep', 'totalTimeInBed',\\\n",
    "                    'totalSleepRecords', \\\n",
    "#                    'mainSlp_duration', \\ # same as mainSlp_timeInBed\n",
    "                   'mainSlp_minutesAsleep',\\\n",
    "                    'mainSlp_timeInBed', \\\n",
    "                    'mainSlp_efficiency', \\\n",
    "                   'mainSlp_restlessDuration',\\\n",
    "                    'mainSlp_restlessCount', \\\n",
    "                    'mainSlp_startMinusMidntMin', \\\n",
    "                   'mainSlp_endMinusMidntMin']\n",
    "\n",
    "# ## NORM \n",
    "# if TO_NORM_PER_PERSON:\n",
    "#     df_slp = norm_per_person_wrapper(df_slp, [\"totalMinutesAsleep\", \"mainSlp_minutesAsleep\",\\\n",
    "#                                               \"mainSlp_efficiency\", \"mainSlp_restlessDuration\", \\\n",
    "#                                               \"mainSlp_startMinusMidntMin\", \"mainSlp_endMinusMidntMin\"\n",
    "#                                              ], \"device_id\")\n",
    "    \n",
    "# ## Add PHASE\n",
    "# df_slp =  add_phase_label_wrapper(df_slp, \"timestamp_dt\")  \n",
    "\n",
    "# # Add week label\n",
    "# df_slp =  add_week_label(df_slp, \"date_dt\", df_survey)  \n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_slp =  add_all_epoch_labels(df_slp, df_survey)  \n",
    "df_slp_in_study = df_slp[(df_slp[\"is_during_study\"]==True)] # helps exclude incomplete days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7019f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_slp.head(2))\n",
    "# print(df_slp.columns)\n",
    "\n",
    "# display(df_slp[\"timestamp_dt\"].head(2))\n",
    "\n",
    "# tmp = df_slp[(df_slp[\"mainSlp_duration\"]!=df_slp[\"mainSlp_timeInBed\"])]\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a338f6",
   "metadata": {},
   "source": [
    "### Sleep - intraday features for tod_lbl AND tod_lbl+dow_lbl\n",
    "- 'totalMinutesAsleep', 'totalTimeInBed', 'totalSleepRecords'\n",
    "- Then for the longest sleep record, get 'mainSlp_minutesAsleep', 'mainSlp_timeInBed',\n",
    "'mainSlp_efficiency', 'mainSlp_restlessDuration', 'mainSlp_restlessCount', \n",
    "'mainSlp_startMinusMidntMin', 'mainSlp_endMinusMidntMin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_FORMAT_FOR_SLEEP_INTRADAY = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def get_time_minus_midnt(dt, dt_midnt):\n",
    "    sub = dt-dt_midnt\n",
    "    return (sub.total_seconds()/60.0)\n",
    "\n",
    "def get_slp_feats_per_day(df_grp):\n",
    "    asleep = df_grp[(df_grp[\"value\"]==\"asleep\")]\n",
    "    restless = df_grp[(df_grp[\"value\"]==\"restless\")]\n",
    "    awake = df_grp[(df_grp[\"value\"]==\"awake\")]\n",
    "    n_recs = len(list(df_grp[\"logId\"].unique()))\n",
    "    df_grp_cnts = (df_grp.groupby(\"logId\")[\"value\"].count()).reset_index()\n",
    "    df_grp_cnts = df_grp_cnts.sort_values(by=[\"value\"], ascending=False)\n",
    "    ## for main only\n",
    "    main_logId = df_grp_cnts.iloc[0][\"logId\"]\n",
    "    df_grp_main_logId = df_grp[(df_grp[\"logId\"]==main_logId)]\n",
    "    main_asleep = df_grp_main_logId[(df_grp_main_logId[\"value\"]==\"asleep\")]\n",
    "    main_restless = df_grp_main_logId[(df_grp_main_logId[\"value\"]==\"restless\")]\n",
    "    main_awake = df_grp_main_logId[(df_grp_main_logId[\"value\"]==\"awake\")]\n",
    "    df_grp_main_logId[\"shift_value\"] = df_grp_main_logId[\"value\"].shift(periods=1)\n",
    "    df_grp_main_logId_value_switches = df_grp_main_logId[(df_grp_main_logId[\"value\"]!=df_grp_main_logId[\"shift_value\"])]\n",
    "    df_grp_main_logId_value_switches_restless = df_grp_main_logId_value_switches[(df_grp_main_logId_value_switches[\"value\"]==\"restless\")]\n",
    "\n",
    "    main_start_in_t_window = datetime.datetime.strptime(df_grp_main_logId[\"datetime\"].iloc[0], DATETIME_FORMAT_FOR_SLEEP_INTRADAY)\n",
    "    main_end_in_t_window = datetime.datetime.strptime(df_grp_main_logId[\"datetime\"].iloc[-1], DATETIME_FORMAT_FOR_SLEEP_INTRADAY)\n",
    "    dt_midnt = datetime.datetime.combine(df_grp.name, datetime.time(0, 0))\n",
    "#     print (dt_midnt)\n",
    "#     print (main_start_in_t_window)\n",
    "    \n",
    "    \n",
    "\n",
    "    return pd.Series({\"asleep\": len(asleep), \\\n",
    "                      \"restless\": len(restless), \\\n",
    "                      \"awake\": len(awake), \\\n",
    "                      \"n_recs\": n_recs, \\\n",
    "                      \"main_logId\": main_logId,\\\n",
    "                      \"main_asleep\": len(main_asleep),\\\n",
    "                      \"main_restless\": len(main_restless),\\\n",
    "                      \"main_awake\": len(main_awake),\\\n",
    "                      \"main_restless_count\": len(df_grp_main_logId_value_switches_restless),\\\n",
    "                      \"main_startMinusMidntMin\": get_time_minus_midnt(main_start_in_t_window, dt_midnt),\\\n",
    "                      \"main_endMinusMidntMin\": get_time_minus_midnt(main_end_in_t_window, dt_midnt),\\\n",
    "                     })\n",
    "\n",
    "\n",
    "\n",
    "def get_slp_intraday_feats_for_grp(df_grp):\n",
    "    feat_dict = {}\n",
    "#     display(df_grp.head(4))\n",
    "#     display(df_grp.iloc[132:137])\n",
    "    df_grp_per_day = df_grp.groupby(\"date_dt\").apply(get_slp_feats_per_day)\n",
    "    df_grp_per_day[\"timeInBed\"] = df_grp_per_day[\"asleep\"] + df_grp_per_day[\"awake\"]\n",
    "    feat_dict[\"totalMinutesAsleep\"] = df_grp_per_day[\"asleep\"].mean()\n",
    "    feat_dict[\"totalTimeInBed\"] = df_grp_per_day[\"timeInBed\"].mean()\n",
    "    feat_dict[\"totalSleepRecords\"] = df_grp_per_day[\"n_recs\"].mean()\n",
    "    ## main\n",
    "    df_grp_per_day[\"main_timeInBed\"] = df_grp_per_day[\"main_asleep\"] + df_grp_per_day[\"main_awake\"]\n",
    "    feat_dict[\"mainSlp_minutesAsleep\"] = df_grp_per_day[\"main_asleep\"].mean()\n",
    "    feat_dict[\"mainSlp_timeInBed\"] = df_grp_per_day[\"main_timeInBed\"].mean()\n",
    "    df_grp_per_day[\"main_efficiency\"] = round((df_grp_per_day[\"main_asleep\"]/df_grp_per_day[\"main_timeInBed\"])*100.0, 0)\n",
    "    feat_dict[\"mainSlp_efficiency\"] = df_grp_per_day[\"main_efficiency\"].mean()\n",
    "    feat_dict[\"mainSlp_restlessDuration\"] = df_grp_per_day[\"main_restless\"].mean()\n",
    "    feat_dict[\"mainSlp_restlessCount\"] = df_grp_per_day[\"main_restless_count\"].mean()\n",
    "    feat_dict[\"mainSlp_startMinusMidntMin\"] = df_grp_per_day[\"main_startMinusMidntMin\"].mean()\n",
    "    feat_dict[\"mainSlp_endMinusMidntMin\"] = df_grp_per_day[\"main_endMinusMidntMin\"].mean()\n",
    "#     display(df_grp_per_day)\n",
    "#     display(feat_dict)\n",
    "    return pd.Series(feat_dict)\n",
    "    \n",
    "\n",
    "def get_slp_intraday_feats_for_did(did, df_survey, grpbycols, only_incld_during_study):\n",
    "    ## (1) did to fitbit id and load parsed csv\n",
    "    fid = FB_MAPPING_DICT_INV[did]\n",
    "    fpath = SLP_INTRADAY_PATH_LIKE.format(fid)\n",
    "    data_df = pd.read_csv(fpath)\n",
    "    data_df[\"device_id\"] = did\n",
    "    data_df[\"timestamp_dt\"] = pd.to_datetime(data_df[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    data_df[\"date_dt\"] = data_df[\"timestamp_dt\"].dt.date\n",
    "    ## (2) add phase, tod, and dow lbls\n",
    "    data_df =  add_all_epoch_labels(data_df, df_survey, print_out=False) \n",
    "    if only_incld_during_study:\n",
    "        data_df = data_df[(data_df[\"is_during_study\"]==True)] # helps exclude incomplete days\n",
    "    #print (len(data_df))\n",
    "    #display(data_df.iloc[30000:30020])\n",
    "    ## (3) function to extract features from group\n",
    "    ## (4) grp by tod_lbl, and grp by tod_lbl and dow_lbl\n",
    "    feats_df_grp = data_df.groupby(grpbycols).apply(get_slp_intraday_feats_for_grp)\n",
    "    feats_df_grp = feats_df_grp.reset_index()\n",
    "#     print (\"features\")\n",
    "#     display(feats_df_grp)\n",
    "    return feats_df_grp\n",
    "    \n",
    "    \n",
    "def get_slp_intraday_feats(did_list, df_survey, grpbycols, only_incld_during_study):\n",
    "    did_df_list = []\n",
    "    cnt = 0\n",
    "    print (\"Processing did cnts intraday...\")\n",
    "    for did in did_list:\n",
    "        cnt += 1\n",
    "        print (\"{0}, \".format(cnt), end='')\n",
    "        did_df = get_slp_intraday_feats_for_did(did, df_survey, grpbycols, only_incld_during_study)\n",
    "        did_df_list.append(did_df)\n",
    "#         break ## debugging\n",
    "    did_df_all = pd.concat(did_df_list)\n",
    "    return (did_df_all)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae4eca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tmp = get_slp_intraday_feats(list(df_slp[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\"])\n",
    "\n",
    "# display (tmp.head(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (len(list(tmp[\"device_id\"].unique())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee321f77",
   "metadata": {},
   "source": [
    "### Sleep - Get per-phase/week features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08860971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slp_phase_feats = df_slp.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "# output_features(df_slp_phase_feats, \"slp\", \"per_phase\")\n",
    "\n",
    "# per-phase\n",
    "df_slp_phase_feats = df_slp_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "output_features(df_slp_phase_feats, \"slp\", \"per_phase\")\n",
    "\n",
    "# per-phase tod -- VALID\n",
    "df_slp_phase_feats = get_slp_intraday_feats(list(df_slp_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\"], only_incld_during_study=True)\n",
    "output_features(df_slp_phase_feats, \"slp\", \"per_phase_tod\")\n",
    "\n",
    "# per-phase dow\n",
    "df_slp_phase_feats = df_slp_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "output_features(df_slp_phase_feats, \"slp\", \"per_phase_dow\")\n",
    "\n",
    "# per-phase tod, dow -- VALID\n",
    "df_slp_phase_feats = get_slp_intraday_feats(list(df_slp_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"], only_incld_during_study=True)\n",
    "output_features(df_slp_phase_feats, \"slp\", \"per_phase_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_slp_phase_feats.head(2))\n",
    "# df_slp_biweekly_feats = df_slp.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "\n",
    "# # biweekly\n",
    "# df_slp_biweekly_feats = df_slp.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "# output_features(df_slp_biweekly_feats, \"slp\", \"biweekly\")\n",
    "# # biweekly tod -- VALID\n",
    "# df_slp_biweekly_feats = get_slp_intraday_feats(list(df_slp[\"device_id\"].unique()), df_survey, [\"device_id\", \"biweekly_lbl\", \"tod_lbl\"])\n",
    "# output_features(df_slp_biweekly_feats, \"slp\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_slp_biweekly_feats = df_slp.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "# output_features(df_slp_biweekly_feats, \"slp\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow -- VALID\n",
    "# df_slp_biweekly_feats = get_slp_intraday_feats(list(df_slp[\"device_id\"].unique()), df_survey, [\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_slp_biweekly_feats, \"slp\", \"biweekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slp_monthly_feats = df_slp.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "\n",
    "# # monthly\n",
    "# df_slp_monthly_feats = df_slp.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "# output_features(df_slp_monthly_feats, \"slp\", \"monthly\")\n",
    "# # monthly tod -- VALID\n",
    "# df_slp_monthly_feats = get_slp_intraday_feats(list(df_slp[\"device_id\"].unique()), df_survey, [\"device_id\", \"monthly_lbl\", \"tod_lbl\"])\n",
    "# output_features(df_slp_monthly_feats, \"slp\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_slp_monthly_feats = df_slp.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "# output_features(df_slp_monthly_feats, \"slp\", \"monthly_dow\")\n",
    "# # monthly tod, dow -- VALID\n",
    "# df_slp_monthly_feats = get_slp_intraday_feats(list(df_slp[\"device_id\"].unique()), df_survey, [\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_slp_monthly_feats, \"slp\", \"monthly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a738c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_slp_biweekly_feats.head(2))\n",
    "# display(df_slp_biweekly_feats.isnull().sum(axis = 0))\n",
    "# display(df_slp_monthly_feats.head(2))\n",
    "# display(df_slp_monthly_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# ## OUTPUT\n",
    "# output_features(df_slp_biweekly_feats, \"msgs\", \"biweekly\")\n",
    "# output_features(df_slp_monthly_feats, \"msgs\", \"monthly\")\n",
    "\n",
    "\n",
    "# # weekly\n",
    "# df_slp_weekly_feats = df_slp.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "# output_features(df_slp_weekly_feats, \"slp\", \"weekly\")\n",
    "# # weekly tod -- VALID\n",
    "# df_slp_weekly_feats = get_slp_intraday_feats(list(df_slp[\"device_id\"].unique()), df_survey, [\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"])\n",
    "# output_features(df_slp_weekly_feats, \"slp\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_slp_weekly_feats = df_slp.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, slp_input_feats))).reset_index()\n",
    "# output_features(df_slp_weekly_feats, \"slp\", \"weekly_dow\")\n",
    "# # weekly tod, dow -- VALID\n",
    "# df_slp_weekly_feats = get_slp_intraday_feats(list(df_slp[\"device_id\"].unique()), df_survey, [\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_slp_weekly_feats, \"slp\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022df474",
   "metadata": {},
   "source": [
    "# Steps - Clean and add phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps = pd.read_csv(STEPS_PATH)\n",
    "df_steps[\"timestamp_dt\"] = pd.to_datetime(df_steps[\"date\"], format=\"%Y-%m-%d\")\n",
    "df_steps[\"date_dt\"] = df_steps[\"timestamp_dt\"].dt.date\n",
    "df_steps[\"device_id\"] = df_steps[\"fitbit_id\"].apply(lambda x: FB_MAPPING_DICT[x])\n",
    "df_steps = df_steps.sort_values(by=[\"device_id\", \"date_dt\"])\n",
    "\n",
    "steps_input_feats = ['steps_total']\n",
    "\n",
    "# ## NORM \n",
    "# if TO_NORM_PER_PERSON:\n",
    "#     df_steps = norm_per_person_wrapper(df_steps, [\"steps_total\"], \"device_id\")\n",
    "\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_steps =  add_phase_label_wrapper(df_steps, \"timestamp_dt\")  \n",
    "\n",
    "# # Add week label\n",
    "# df_steps =  add_week_label(df_steps, \"date_dt\", df_survey)  \n",
    "\n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_steps =  add_all_epoch_labels(df_steps, df_survey)  \n",
    "df_steps_in_study = df_steps[(df_steps[\"is_during_study\"]==True)] # helps exclude incomplete days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c12624",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_steps.iloc[100:160])\n",
    "print(df_steps.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps_levels = pd.read_csv(STEPS_LEVELS_PATH)\n",
    "df_steps_levels[\"timestamp_dt\"] = pd.to_datetime(df_steps_levels[\"date\"], format=\"%Y-%m-%d\")\n",
    "df_steps_levels[\"date_dt\"] = df_steps_levels[\"timestamp_dt\"].dt.date\n",
    "df_steps_levels[\"device_id\"] = df_steps_levels[\"fitbit_id\"].apply(lambda x: FB_MAPPING_DICT[x])\n",
    "df_steps_levels = df_steps_levels.sort_values(by=[\"device_id\", \"date_dt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_levels_input_feats = ['veryActiveMinutes', 'fairlyActiveMinutes', 'lightlyActiveMinutes', 'sedentaryMinutes']\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_steps_levels =  add_phase_label_wrapper(df_steps_levels, \"timestamp_dt\")  \n",
    "# # Add week label\n",
    "# df_steps_levels =  add_week_label(df_steps_levels, \"date_dt\", df_survey)  \n",
    "\n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_steps_levels =  add_all_epoch_labels(df_steps_levels, df_survey)  \n",
    "df_steps_levels_in_study = df_steps_levels[(df_steps_levels[\"is_during_study\"]==True)] # helps exclude incomplete days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_steps_levels.head(2))\n",
    "print(df_steps_levels.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b87b1",
   "metadata": {},
   "source": [
    "### Steps and steps levels - intraday features for tod_lbl AND tod_lbl+dow_lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eea65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps_intraday_feats_for_grp(df_grp):\n",
    "#     print (df_grp.name)\n",
    "#     display(df_grp.head(10))\n",
    "    feat_dict = {}\n",
    "#     display(df_grp.head(4))\n",
    "#     display(df_grp.iloc[132:137])\n",
    "    df_grp_per_day = pd.DataFrame(df_grp.groupby(\"date_dt\")[\"value\"].sum())\n",
    "#     display(df_grp_per_day)\n",
    "    feat_dict[\"value\"] = df_grp_per_day[\"value\"].mean()\n",
    "    return pd.Series(feat_dict)\n",
    "    \n",
    "    \n",
    "def add_did_ts_epoch_cols(data_df, did, only_incld_during_study):\n",
    "    data_df[\"device_id\"] = did\n",
    "    data_df[\"timestamp_dt\"] = pd.to_datetime(data_df[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    data_df[\"date_dt\"] = data_df[\"timestamp_dt\"].dt.date\n",
    "    ## (2) add phase, tod, and dow lbls\n",
    "    data_df =  add_all_epoch_labels(data_df, df_survey, print_out=False)\n",
    "    if only_incld_during_study:\n",
    "        data_df = data_df[(data_df[\"is_during_study\"]==True)] # helps exclude incomplete days\n",
    "    return data_df\n",
    "\n",
    "def get_steps_intraday_feats_for_did(did, df_survey, grpbycols, only_incld_during_study):\n",
    "    ## (1) did to fitbit id and load parsed csv\n",
    "    fid = FB_MAPPING_DICT_INV[did]\n",
    "    steps_df = add_did_ts_epoch_cols(pd.read_csv(STEPS_INTRADAY_PATH_LIKE.format(fid)), did, only_incld_during_study)\n",
    "    very_act_df = add_did_ts_epoch_cols(pd.read_csv(VERY_ACT_INTRADAY_PATH_LIKE.format(fid)), did, only_incld_during_study)\n",
    "    fairly_act_df = add_did_ts_epoch_cols(pd.read_csv(FAIRLY_ACT_INTRADAY_PATH_LIKE.format(fid)), did, only_incld_during_study)\n",
    "    lightly_act_df = add_did_ts_epoch_cols(pd.read_csv(LIGHTLY_ACT_INTRADAY_PATH_LIKE.format(fid)), did, only_incld_during_study)\n",
    "    sed_act_df = add_did_ts_epoch_cols(pd.read_csv(SED_ACT_INTRADAY_PATH_LIKE.format(fid)), did, only_incld_during_study)\n",
    "#     print (\"intraday loaded\")\n",
    "    feats = [\"steps_total\", \"veryActiveMinutes\", \"fairlyActiveMinutes\", \"lightlyActiveMinutes\", \"sedentaryMinutes\"]\n",
    "    data_df_for_feats = [steps_df, very_act_df, fairly_act_df, lightly_act_df, sed_act_df]\n",
    "#     feats = [\"veryActiveMinutes\"]\n",
    "#     data_df_for_feats = [very_act_df]\n",
    "    data_df_feats_list = []\n",
    "    for ki in range(0, len(feats)):\n",
    "        feat_name = feats[ki]\n",
    "        data_df = data_df_for_feats[ki]\n",
    "        data_df_feats = data_df.groupby(grpbycols).apply(get_steps_intraday_feats_for_grp)\n",
    "        data_df_feats = data_df_feats.rename(columns={\"value\": feat_name})\n",
    "        #display(data_df_feats)\n",
    "        data_df_feats_list.append(data_df_feats)\n",
    "    data_df_feats_all = (pd.concat(data_df_feats_list, axis=1)).reset_index()\n",
    "#     display(data_df_feats_all)\n",
    "    return data_df_feats_all\n",
    "    \n",
    "    \n",
    "def get_steps_intraday_feats(did_list, df_survey, grpbycols, only_incld_during_study):\n",
    "    did_df_list = []\n",
    "    cnt = 0\n",
    "    print (\"Processing did cnts intraday...\")\n",
    "    for did in did_list:\n",
    "        cnt += 1\n",
    "        print (\"{0}, \".format(cnt), end='')\n",
    "        did_df = get_steps_intraday_feats_for_did(did, df_survey, grpbycols, only_incld_during_study)\n",
    "        did_df_list.append(did_df)\n",
    "#         break ## debugging\n",
    "    did_df_all = pd.concat(did_df_list)\n",
    "    return (did_df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = get_steps_intraday_feats(list(df_steps[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(tmp.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc135c5",
   "metadata": {},
   "source": [
    "### Steps and steps levels - Get per-phase/week features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccac631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per phase\n",
    "df_steps_phase_feats = df_steps_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "df_steps_levels_phase_feats = df_steps_levels_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "df_steps_phase_feats = pd.merge(df_steps_phase_feats, df_steps_levels_phase_feats,  how='left', left_on=['device_id','phase_wrapper'], right_on = ['device_id','phase_wrapper'])\n",
    "output_features(df_steps_phase_feats, \"steps\", \"per_phase\")\n",
    "\n",
    "\n",
    "# per phase dow\n",
    "df_steps_phase_feats = df_steps_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "df_steps_levels_phase_feats = df_steps_levels_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "df_steps_phase_feats = pd.merge(df_steps_phase_feats, df_steps_levels_phase_feats,  how='left', left_on=['device_id','phase_wrapper', \"dow_lbl\"], right_on = ['device_id','phase_wrapper', \"dow_lbl\"])\n",
    "output_features(df_steps_phase_feats, \"steps\", \"per_phase_dow\")\n",
    "\n",
    "\n",
    "\n",
    "# # per phase tod -- INVALID\n",
    "# df_steps_phase_feats = df_steps.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_phase_feats = df_steps_levels.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_phase_feats = pd.merge(df_steps_phase_feats, df_steps_levels_phase_feats,  how='left', left_on=['device_id','phase_wrapper', \"tod_lbl\"], right_on = ['device_id','phase_wrapper', \"tod_lbl\"])\n",
    "# output_features(df_steps_phase_feats, \"steps\", \"per_phase_tod\")\n",
    "\n",
    "# # per phase tod dow -- INVALID\n",
    "# df_steps_phase_feats = df_steps.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_phase_feats = df_steps_levels.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_phase_feats = pd.merge(df_steps_phase_feats, df_steps_levels_phase_feats,  how='left', left_on=['device_id','phase_wrapper', \"tod_lbl\", \"dow_lbl\"], right_on = ['device_id','phase_wrapper', \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_steps_phase_feats, \"steps\", \"per_phase_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## per phase tod\n",
    "df_steps_phase_feats = get_steps_intraday_feats(list(df_steps_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\"], only_incld_during_study=True)\n",
    "output_features(df_steps_phase_feats, \"steps\", \"per_phase_tod\")\n",
    "\n",
    "## per phase tod dow\n",
    "df_steps_phase_feats = get_steps_intraday_feats(list(df_steps_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"], only_incld_during_study=True)\n",
    "output_features(df_steps_phase_feats, \"steps\", \"per_phase_tod_dow\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # biweekly\n",
    "# df_steps_biweekly_feats = df_steps.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_biweekly_feats = df_steps_levels.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_biweekly_feats = pd.merge(df_steps_biweekly_feats, df_steps_levels_biweekly_feats,  how='left', left_on=['device_id','biweekly_lbl'], right_on = ['device_id','biweekly_lbl'])\n",
    "# output_features(df_steps_biweekly_feats, \"steps\", \"biweekly_lbl\")\n",
    "\n",
    "# # biweekly tod\n",
    "# df_steps_biweekly_feats = df_steps.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_biweekly_feats = df_steps_levels.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_biweekly_feats = pd.merge(df_steps_biweekly_feats, df_steps_levels_biweekly_feats,  how='left', left_on=['device_id','biweekly_lbl', \"tod_lbl\"], right_on = ['device_id','biweekly_lbl', \"tod_lbl\"])\n",
    "# output_features(df_steps_biweekly_feats, \"steps\", \"per_biweekly_tod\")\n",
    "\n",
    "# # biweekly dow\n",
    "# df_steps_biweekly_feats = df_steps.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_biweekly_feats = df_steps_levels.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_biweekly_feats = pd.merge(df_steps_biweekly_feats, df_steps_levels_biweekly_feats,  how='left', left_on=['device_id','biweekly_lbl', \"dow_lbl\"], right_on = ['device_id','biweekly_lbl', \"dow_lbl\"])\n",
    "# output_features(df_steps_biweekly_feats, \"steps\", \"per_biweekly_dow\")\n",
    "\n",
    "# # biweekly tod\n",
    "# df_steps_biweekly_feats = df_steps.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_biweekly_feats = df_steps_levels.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_biweekly_feats = pd.merge(df_steps_biweekly_feats, df_steps_levels_biweekly_feats,  how='left', left_on=['device_id','biweekly_lbl', \"tod_lbl\", \"dow_lbl\"], right_on = ['device_id','biweekly_lbl', \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_steps_biweekly_feats, \"steps\", \"per_biweekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # monthly\n",
    "# df_steps_monthly_feats = df_steps.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_monthly_feats = df_steps_levels.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_monthly_feats = pd.merge(df_steps_monthly_feats, df_steps_levels_monthly_feats,  how='left', left_on=['device_id','monthly_lbl'], right_on = ['device_id','monthly_lbl'])\n",
    "# output_features(df_steps_monthly_feats, \"steps\", \"monthly_lbl\")\n",
    "\n",
    "# # monthly tod\n",
    "# df_steps_monthly_feats = df_steps.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_monthly_feats = df_steps_levels.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_monthly_feats = pd.merge(df_steps_monthly_feats, df_steps_levels_monthly_feats,  how='left', left_on=['device_id','monthly_lbl', \"tod_lbl\"], right_on = ['device_id','monthly_lbl', \"tod_lbl\"])\n",
    "# output_features(df_steps_monthly_feats, \"steps\", \"per_monthly_tod\")\n",
    "\n",
    "# # monthly dow\n",
    "# df_steps_monthly_feats = df_steps.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_monthly_feats = df_steps_levels.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_monthly_feats = pd.merge(df_steps_monthly_feats, df_steps_levels_monthly_feats,  how='left', left_on=['device_id','monthly_lbl', \"dow_lbl\"], right_on = ['device_id','monthly_lbl', \"dow_lbl\"])\n",
    "# output_features(df_steps_monthly_feats, \"steps\", \"per_monthly_dow\")\n",
    "\n",
    "# # monthly tod\n",
    "# df_steps_monthly_feats = df_steps.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_monthly_feats = df_steps_levels.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_monthly_feats = pd.merge(df_steps_monthly_feats, df_steps_levels_monthly_feats,  how='left', left_on=['device_id','monthly_lbl', \"tod_lbl\", \"dow_lbl\"], right_on = ['device_id','monthly_lbl', \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_steps_monthly_feats, \"steps\", \"per_monthly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weekly\n",
    "# df_steps_weekly_feats = df_steps.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_weekly_feats = df_steps_levels.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_weekly_feats = pd.merge(df_steps_weekly_feats, df_steps_levels_weekly_feats,  how='left', left_on=['device_id','wknum_wrt_wk12'], right_on = ['device_id','wknum_wrt_wk12'])\n",
    "# output_features(df_steps_weekly_feats, \"steps\", \"weekly\")\n",
    "\n",
    "# # weekly tod\n",
    "# df_steps_weekly_feats = df_steps.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_weekly_feats = df_steps_levels.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_weekly_feats = pd.merge(df_steps_weekly_feats, df_steps_levels_weekly_feats,  how='left', left_on=['device_id','wknum_wrt_wk12', \"tod_lbl\"], right_on = ['device_id','wknum_wrt_wk12', \"tod_lbl\"])\n",
    "# output_features(df_steps_weekly_feats, \"steps\", \"weekly_tod\")\n",
    "\n",
    "# # weekly dow\n",
    "# df_steps_weekly_feats = df_steps.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_weekly_feats = df_steps_levels.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_weekly_feats = pd.merge(df_steps_weekly_feats, df_steps_levels_weekly_feats,  how='left', left_on=['device_id','wknum_wrt_wk12', \"dow_lbl\"], right_on = ['device_id','wknum_wrt_wk12', \"dow_lbl\"])\n",
    "# output_features(df_steps_weekly_feats, \"steps\", \"weekly_dow\")\n",
    "\n",
    "# # weekly tod\n",
    "# df_steps_weekly_feats = df_steps.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "# df_steps_levels_weekly_feats = df_steps_levels.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "# df_steps_weekly_feats = pd.merge(df_steps_weekly_feats, df_steps_levels_weekly_feats,  how='left', left_on=['device_id','wknum_wrt_wk12', \"tod_lbl\", \"dow_lbl\"], right_on = ['device_id','wknum_wrt_wk12', \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_steps_weekly_feats, \"steps\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_steps_phase_feats.head(2))\n",
    "# # display(df_steps_levels_phase_feats.head(2))\n",
    "\n",
    "# display(df_steps_phase_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# display(df_steps_phase_feats[(df_steps_phase_feats[\"veryActiveMinutes\"].isnull())])\n",
    "\n",
    "\n",
    "# ## OUTPUT\n",
    "# output_features(df_steps_phase_feats, \"steps\", \"per_phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_steps_biweekly_feats = df_steps.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "\n",
    "# df_steps_levels_biweekly_feats = df_steps_levels.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "\n",
    "\n",
    "# df_steps_biweekly_feats = pd.merge(df_steps_biweekly_feats, df_steps_levels_biweekly_feats,  how='left', left_on=['device_id','phase_wrapper'], right_on = ['device_id','phase_wrapper'])\n",
    "\n",
    "\n",
    "\n",
    "# df_steps_monthly_feats = df_steps.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, steps_input_feats))).reset_index()\n",
    "\n",
    "# df_steps_levels_monthly_feats = df_steps_levels.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, steps_levels_input_feats))).reset_index()\n",
    "\n",
    "\n",
    "# df_steps_monthly_feats = pd.merge(df_steps_monthly_feats, df_steps_levels_monthly_feats,  how='left', left_on=['device_id','phase_wrapper'], right_on = ['device_id','phase_wrapper'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_steps_biweekly_feats.head(2))\n",
    "\n",
    "# display(df_steps_biweekly_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# display(df_steps_biweekly_feats[(df_steps_biweekly_feats[\"veryActiveMinutes\"].isnull())])\n",
    "\n",
    "\n",
    "# display(df_steps_monthly_feats.head(2))\n",
    "\n",
    "# display(df_steps_monthly_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# display(df_steps_monthly_feats[(df_steps_monthly_feats[\"veryActiveMinutes\"].isnull())])\n",
    "\n",
    "\n",
    "\n",
    "# ## OUTPUT\n",
    "# output_features(df_steps_biweekly_feats, \"steps\", \"biweekly\")\n",
    "# output_features(df_steps_monthly_feats, \"steps\", \"monthly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f66df",
   "metadata": {},
   "source": [
    "# Loc - Clean and add phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc428e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing did counts (tweak)...\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, Processing did counts...\n",
      "Processing did counts...\n"
     ]
    }
   ],
   "source": [
    "df_loc = pd.read_csv(LOC_PATH)\n",
    "df_loc[\"timestamp_dt\"] = df_loc[\"timestamp\"].apply(sensor_time_to_local)\n",
    "df_loc[\"date_dt\"] = df_loc[\"timestamp_dt\"].dt.date\n",
    "df_loc = df_loc.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_loc =  add_phase_label_wrapper(df_loc, \"timestamp_dt\")  \n",
    "\n",
    "# # Add week label\n",
    "# df_loc =  add_week_label(df_loc, \"date_dt\", df_survey) \n",
    "\n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_loc =  add_all_epoch_labels(df_loc, df_survey)  \n",
    "df_loc_in_study = df_loc[(df_loc[\"is_during_study\"]==True)] # helps exclude incomplete days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd471a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime_EST', '_id', 'timestamp', 'device_id', 'latitude',\n",
      "       'longitude', 'stationary', 'time_label', 'location_label',\n",
      "       'semantic_loc', 'timestamp_dt', 'date_dt', 'phase', 'phase_wrapper',\n",
      "       'biweekly_lbl', 'monthly_lbl', 'biweekly_date', 'monthly_date', 'dow',\n",
      "       'dow_lbl', 'hour_dt', 'tod_lbl', 'wknum_wrt_wk12', 'is_during_study'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['other', 'transit', 'home', 'work'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(df_loc.head(20))\n",
    "print(df_loc.columns)\n",
    "\n",
    "display(df_loc[\"semantic_loc\"].unique())\n",
    "display(df_loc[\"stationary\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd261f",
   "metadata": {},
   "source": [
    "### Loc - feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3deca359",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOC_DIDS = []\n",
    "def add_features_loc(g):\n",
    "    if \"device_id\" in g.columns:\n",
    "        did = g[\"device_id\"].iloc[0]\n",
    "    else:\n",
    "        did = g.name[0]\n",
    "    if did not in LOC_DIDS:\n",
    "        LOC_DIDS.append(did)\n",
    "        print (\"{0}, \".format(len(LOC_DIDS)), end='')\n",
    "    feats_dict = {}\n",
    "    feats_dict[\"number_of_clusters\"] = number_of_clusters(g)\n",
    "    feats_dict[\"home_stay_time_percent\"] = home_stay_time_percent(g)\n",
    "    feats_dict[\"radius_of_gyration\"] = radius_of_gyration(g)\n",
    "    len_stay_clust_series = len_stay_at_clusters_in_minutes(g, SAMPLE_RATE=5)\n",
    "    feats_dict[\"std_len_stay_at_clusters_in_minutes\"] = len_stay_clust_series[\"std_len_stay_at_clusters_in_minutes\"]\n",
    "    feats_dict[\"mean_len_stay_at_clusters_in_minutes\"] = len_stay_clust_series[\"mean_len_stay_at_clusters_in_minutes\"]\n",
    "    feats_dict[\"pct_time_at_top_cluster_1\"] = pct_time_at_top_cluster_x(g, 1)\n",
    "    feats_dict[\"pct_time_at_top_cluster_2\"] = pct_time_at_top_cluster_x(g, 2)\n",
    "    feats_dict[\"pct_time_at_top_cluster_3\"] = pct_time_at_top_cluster_x(g, 3)\n",
    "\n",
    "    feats_dict[\"circadian_movement\"] = circadian_movement(g)\n",
    "    feats_dict[\"location_entropy\"] = location_entropy(g)\n",
    "    feats_dict[\"location_entropy_normalized\"] = location_entropy_normalized(g)\n",
    "    feats_dict[\"location_variance\"] = location_variance(g)\n",
    "    feats_dict[\"location_variance_log\"] = location_variance_log(g)\n",
    "    \n",
    "#     feats_dict[\"number_location_transitions\"] = number_location_transitions(g) -- no need\n",
    "\n",
    "    feats_dict[\"total_distance\"] = travel_distance_meters(g, SAMPLE_RATE=10) # sample rate is 5, but we can include distances that are 10 min apart.\n",
    "    feats_dict[\"moving_time_percent\"] = moving_time_percent(g)\n",
    "    feats_dict[\"outliers_time_percent\"] = outliers_time_percent(g)\n",
    "    return pd.Series(feats_dict)\n",
    "\n",
    "\n",
    "def mean_loc_feats_across_days(df_grp):\n",
    "    df_grp_feats_per_day = df_grp.groupby(\"date_dt\", as_index=False).apply((lambda x: add_features_loc(x))).reset_index(drop=True)\n",
    "    df_grp_feats_per_day_mean = df_grp_feats_per_day.mean().to_frame().transpose()\n",
    "#     display(df_grp_feats_per_day_mean)\n",
    "    return df_grp_feats_per_day_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8057bc",
   "metadata": {},
   "source": [
    "### Locs - Get per-phase/week features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82abe1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = df_loc.iloc[0:50000].groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f114e7e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, "
     ]
    }
   ],
   "source": [
    "##per phase - use only complete days so that mean is not skewed\n",
    "##we are averaging across days as people may have different number of days in each phase\n",
    "df_loc_phase_feats = df_loc_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: mean_loc_feats_across_days(x))).reset_index()\n",
    "df_loc_phase_feats = df_loc_phase_feats.drop(columns=[\"level_2\"])\n",
    "output_features(df_loc_phase_feats, \"loc\", \"per_phase\")\n",
    "\n",
    "##per-phase tod\n",
    "df_loc_phase_feats = df_loc_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: mean_loc_feats_across_days(x))).reset_index()\n",
    "df_loc_phase_feats = df_loc_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_loc_phase_feats, \"loc\", \"per_phase_tod\")\n",
    "\n",
    "##per-phase dow\n",
    "df_loc_phase_feats = df_loc_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: mean_loc_feats_across_days(x))).reset_index()\n",
    "df_loc_phase_feats = df_loc_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_loc_phase_feats, \"loc\", \"per_phase_dow\")\n",
    "\n",
    "##per-phase tod, dow\n",
    "df_loc_phase_feats = df_loc_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: mean_loc_feats_across_days(x))).reset_index()\n",
    "df_loc_phase_feats = df_loc_phase_feats.drop(columns=[\"level_4\"])\n",
    "output_features(df_loc_phase_feats, \"loc\", \"per_phase_tod_dow\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # per-phase\n",
    "# df_loc_phase_feats = df_loc.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_phase_feats, \"loc\", \"per_phase\")\n",
    "# # per-phase tod\n",
    "# df_loc_phase_feats = df_loc.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_phase_feats, \"loc\", \"per_phase_tod\")\n",
    "# # per-phase dow\n",
    "# df_loc_phase_feats = df_loc.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_phase_feats, \"loc\", \"per_phase_dow\")\n",
    "# # per-phase tod, dow\n",
    "# df_loc_phase_feats = df_loc.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_phase_feats, \"loc\", \"per_phase_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd50a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # biweekly\n",
    "# df_loc_biweekly_feats = df_loc.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_biweekly_feats, \"loc\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_loc_biweekly_feats = df_loc.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_biweekly_feats, \"loc\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_loc_biweekly_feats = df_loc.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_biweekly_feats, \"loc\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow\n",
    "# df_loc_biweekly_feats = df_loc.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_biweekly_feats, \"loc\", \"biweekly_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # monthly\n",
    "# df_loc_monthly_feats = df_loc.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_monthly_feats, \"loc\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_loc_monthly_feats = df_loc.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_monthly_feats, \"loc\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_loc_monthly_feats = df_loc.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_monthly_feats, \"loc\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_loc_monthly_feats = df_loc.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_monthly_feats, \"loc\", \"monthly_tod_dow\")\n",
    "\n",
    "\n",
    "# df_loc_biweekly_feats = df_loc.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# df_loc_monthly_feats = df_loc.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_loc_biweekly_feats.head(2))\n",
    "# display(df_loc_monthly_feats.head(2))\n",
    "\n",
    "# # OUTPUT\n",
    "# output_features(df_loc_biweekly_feats, \"loc\", \"biweekly\")\n",
    "# output_features(df_loc_monthly_feats, \"loc\", \"monthly\")\n",
    "\n",
    "\n",
    "# # weekly\n",
    "# df_loc_weekly_feats = df_loc.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_weekly_feats, \"loc\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_loc_weekly_feats = df_loc.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_weekly_feats, \"loc\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_loc_weekly_feats = df_loc.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_weekly_feats, \"loc\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_loc_weekly_feats = df_loc.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_loc(x))).reset_index()\n",
    "# output_features(df_loc_weekly_feats, \"loc\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e3ddc",
   "metadata": {},
   "source": [
    "# Wifi - Clean and add phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wifi = pd.read_csv(WIFI_PATH)\n",
    "df_wifi[\"timestamp_dt\"] = df_wifi[\"timestamp\"].apply(sensor_time_to_local)\n",
    "df_wifi[\"date_dt\"] = df_wifi[\"timestamp_dt\"].dt.date\n",
    "df_wifi = df_wifi.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_wifi =  add_phase_label_wrapper(df_wifi, \"timestamp_dt\")  \n",
    "\n",
    "# # Add week label\n",
    "# df_wifi =  add_week_label(df_wifi, \"date_dt\", df_survey)  \n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_wifi =  add_all_epoch_labels(df_wifi, df_survey)  \n",
    "df_wifi_in_study = df_wifi[(df_wifi[\"is_during_study\"]==True)] # helps exclude incomplete days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_wifi.head(2))\n",
    "print(df_wifi.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d087c",
   "metadata": {},
   "source": [
    "### Wifi - feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_wifi(g):\n",
    "    feats_dict = {}\n",
    "    feats_dict[\"number_unique_wifi_hotspots\"] = number_unique_wifi_hotspots(g)\n",
    "    return pd.Series(feats_dict)\n",
    "\n",
    "def mean_wifi_feats_across_days(df_grp):\n",
    "    df_grp_feats_per_day = df_grp.groupby(\"date_dt\", as_index=False).apply((lambda x: add_features_wifi(x))).reset_index(drop=True)\n",
    "    df_grp_feats_per_day_mean = df_grp_feats_per_day.mean().to_frame().transpose()\n",
    "#     display(df_grp_feats_per_day_mean)\n",
    "    return df_grp_feats_per_day_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e62e5",
   "metadata": {},
   "source": [
    "### Wifi - Get per-phase/week features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c20aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per phase - use only complete days so that mean is not skewed\n",
    "# we are averaging across days as people may have different number of days in each phase\n",
    "# df_wifi_phase_feats = df_wifi_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: mean_wifi_feats_across_days(x))).reset_index()\n",
    "# df_wifi_phase_feats = df_wifi_phase_feats.drop(columns=[\"level_2\"])\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase\")\n",
    "\n",
    "# per-phase tod\n",
    "# df_wifi_phase_feats = df_wifi_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: mean_wifi_feats_across_days(x))).reset_index()\n",
    "# df_wifi_phase_feats = df_wifi_phase_feats.drop(columns=[\"level_3\"])\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase_tod\")\n",
    "\n",
    "# per-phase dow\n",
    "# df_wifi_phase_feats = df_wifi_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: mean_wifi_feats_across_days(x))).reset_index()\n",
    "# df_wifi_phase_feats = df_wifi_phase_feats.drop(columns=[\"level_3\"])\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase_dow\")\n",
    "\n",
    "# per-phase tod, dow\n",
    "# df_wifi_phase_feats = df_wifi_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: mean_wifi_feats_across_days(x))).reset_index()\n",
    "# df_wifi_phase_feats = df_wifi_phase_feats.drop(columns=[\"level_4\"])\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase_tod_dow\")\n",
    "\n",
    "\n",
    "\n",
    "# # per-phase\n",
    "# df_wifi_phase_feats = df_wifi.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase\")\n",
    "# # per-phase tod\n",
    "# df_wifi_phase_feats = df_wifi.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase_tod\")\n",
    "# # per-phase dow\n",
    "# df_wifi_phase_feats = df_wifi.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase_dow\")\n",
    "# # per-phase tod, dow\n",
    "# df_wifi_phase_feats = df_wifi.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_phase_feats, \"wifi\", \"per_phase_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44753fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # biweekly\n",
    "# df_wifi_biweekly_feats = df_wifi.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_biweekly_feats, \"wifi\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_wifi_biweekly_feats = df_wifi.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_biweekly_feats, \"wifi\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_wifi_biweekly_feats = df_wifi.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_biweekly_feats, \"wifi\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow\n",
    "# df_wifi_biweekly_feats = df_wifi.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_biweekly_feats, \"wifi\", \"biweekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81531c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_wifi_biweekly_feats = df_wifi.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# df_wifi_monthly_feats = df_wifi.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "\n",
    "# # monthly\n",
    "# df_wifi_monthly_feats = df_wifi.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_monthly_feats, \"wifi\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_wifi_monthly_feats = df_wifi.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_monthly_feats, \"wifi\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_wifi_monthly_feats = df_wifi.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_monthly_feats, \"wifi\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_wifi_monthly_feats = df_wifi.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_monthly_feats, \"wifi\", \"monthly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_wifi_biweekly_feats.head(2))\n",
    "# display(df_wifi_monthly_feats.head(2))\n",
    "\n",
    "# # # OUTPUT\n",
    "# # output_features(df_wifi_biweekly_feats, \"wifi\", \"biweekly\")\n",
    "# # output_features(df_wifi_monthly_feats, \"wifi\", \"monthly\")\n",
    "\n",
    "\n",
    "# # weekly\n",
    "# df_wifi_weekly_feats = df_wifi.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_weekly_feats, \"wifi\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_wifi_weekly_feats = df_wifi.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_weekly_feats, \"wifi\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_wifi_weekly_feats = df_wifi.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_weekly_feats, \"wifi\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_wifi_weekly_feats = df_wifi.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_wifi(x))).reset_index()\n",
    "# output_features(df_wifi_weekly_feats, \"wifi\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe38f5",
   "metadata": {},
   "source": [
    "# Screen - Clean and add phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fac230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scr = pd.read_csv(SCR_PATH)\n",
    "df_scr[\"timestamp_dt\"] = df_scr[\"timestamp\"].apply(sensor_time_to_local)\n",
    "df_scr[\"date_dt\"] = df_scr[\"timestamp_dt\"].dt.date\n",
    "df_scr = df_scr.sort_values(by=[\"device_id\", \"timestamp_dt\"])\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_scr =  add_phase_label_wrapper(df_scr, \"timestamp_dt\")  \n",
    "\n",
    "# # Add week label\n",
    "# df_scr =  add_week_label(df_scr, \"date_dt\", df_survey)  \n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_scr =  add_all_epoch_labels(df_scr, df_survey)  \n",
    "df_scr_in_study = df_scr[(df_scr[\"is_during_study\"]==True)] # helps exclude incomplete days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbefc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_scr.head(2))\n",
    "print(df_scr.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb467c8",
   "metadata": {},
   "source": [
    "### Screen - feature function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_scr(g):\n",
    "    feats_dict = {}\n",
    "    feats_dict[\"number_of_unlocks\"] = number_of_unlocks(g)\n",
    "    feats_dict[\"mean_unlocks_per_minute\"] = mean_unlocks_per_minute(g)\n",
    "#     feats_dict[\"median_unlocks_per_minute\"] = median_unlocks_per_minute(g)\n",
    "    feats_dict[\"interaction_time_minutes\"] = interaction_time_minutes(g)\n",
    "#     feats_dict[\"mean_interaction_time_per_use_secs\"] = mean_interaction_time_per_use_secs(g)\n",
    "    feats_dict[\"median_interaction_time_per_use_secs\"] = median_interaction_time_per_use_secs(g)\n",
    "    return pd.Series(feats_dict)\n",
    "\n",
    "def mean_scr_feats_across_days(df_grp):\n",
    "    df_grp_feats_per_day = df_grp.groupby(\"date_dt\", as_index=False).apply((lambda x: add_features_scr(x))).reset_index(drop=True)\n",
    "    df_grp_feats_per_day_mean = df_grp_feats_per_day.mean().to_frame().transpose()\n",
    "#     display(df_grp_feats_per_day_mean)\n",
    "    return df_grp_feats_per_day_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0f1c3",
   "metadata": {},
   "source": [
    "### Scr - Get per-phase features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "##per phase - use only complete days so that mean is not skewed\n",
    "##we are averaging across days as people may have different number of days in each phase\n",
    "df_scr_phase_feats = df_scr_in_study.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: mean_scr_feats_across_days(x))).reset_index()\n",
    "df_scr_phase_feats = df_scr_phase_feats.drop(columns=[\"level_2\"])\n",
    "output_features(df_scr_phase_feats, \"scr\", \"per_phase\")\n",
    "\n",
    "##per-phase tod\n",
    "df_scr_phase_feats = df_scr_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: mean_scr_feats_across_days(x))).reset_index()\n",
    "df_scr_phase_feats = df_scr_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_scr_phase_feats, \"scr\", \"per_phase_tod\")\n",
    "\n",
    "##per-phase dow\n",
    "df_scr_phase_feats = df_scr_in_study.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: mean_scr_feats_across_days(x))).reset_index()\n",
    "df_scr_phase_feats = df_scr_phase_feats.drop(columns=[\"level_3\"])\n",
    "output_features(df_scr_phase_feats, \"scr\", \"per_phase_dow\")\n",
    "\n",
    "##per-phase tod, dow\n",
    "df_scr_phase_feats = df_scr_in_study.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: mean_scr_feats_across_days(x))).reset_index()\n",
    "df_scr_phase_feats = df_scr_phase_feats.drop(columns=[\"level_4\"])\n",
    "output_features(df_scr_phase_feats, \"scr\", \"per_phase_tod_dow\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # per-phase\n",
    "# df_scr_phase_feats = df_scr.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_phase_feats, \"scr\", \"per_phase\")\n",
    "# # per-phase tod\n",
    "# df_scr_phase_feats = df_scr.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_phase_feats, \"scr\", \"per_phase_tod\")\n",
    "# # per-phase dow\n",
    "# df_scr_phase_feats = df_scr.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_phase_feats, \"scr\", \"per_phase_dow\")\n",
    "# # per-phase tod, dow\n",
    "# df_scr_phase_feats = df_scr.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_phase_feats, \"scr\", \"per_phase_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_scr_biweekly_feats = df_scr.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# # df_scr_monthly_feats = df_scr.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "\n",
    "# # biweekly\n",
    "# df_scr_biweekly_feats = df_scr.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_biweekly_feats, \"scr\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_scr_biweekly_feats = df_scr.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_biweekly_feats, \"scr\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_scr_biweekly_feats = df_scr.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_biweekly_feats, \"scr\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow\n",
    "# df_scr_biweekly_feats = df_scr.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_biweekly_feats, \"scr\", \"biweekly_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d907da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # monthly\n",
    "# df_scr_monthly_feats = df_scr.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_monthly_feats, \"scr\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_scr_monthly_feats = df_scr.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_monthly_feats, \"scr\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_scr_monthly_feats = df_scr.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_monthly_feats, \"scr\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_scr_monthly_feats = df_scr.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_monthly_feats, \"scr\", \"monthly_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weekly\n",
    "# df_scr_weekly_feats = df_scr.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_weekly_feats, \"scr\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_scr_weekly_feats = df_scr.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_weekly_feats, \"scr\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_scr_weekly_feats = df_scr.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_weekly_feats, \"scr\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_scr_weekly_feats = df_scr.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_scr(x))).reset_index()\n",
    "# output_features(df_scr_weekly_feats, \"scr\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22004cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_scr_biweekly_feats.head(2))\n",
    "# display(df_scr_biweekly_feats.isnull().sum(axis = 0))\n",
    "# display(df_scr_monthly_feats.head(2))\n",
    "# display(df_scr_monthly_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# # OUTPUT\n",
    "# output_features(df_scr_biweekly_feats, \"scr\", \"biweekly\")\n",
    "# output_features(df_scr_monthly_feats, \"scr\", \"monthly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189179d7",
   "metadata": {},
   "source": [
    "# HR - Clean and add phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7610fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr = pd.read_csv(HR_PATH)\n",
    "df_hr[\"timestamp_dt\"] = pd.to_datetime(df_hr[\"date\"], format=\"%Y-%m-%d\")\n",
    "df_hr[\"date_dt\"] = df_hr[\"timestamp_dt\"].dt.date\n",
    "df_hr[\"device_id\"] = df_hr[\"fitbit_id\"].apply(lambda x: FB_MAPPING_DICT[x])\n",
    "df_hr = df_hr.sort_values(by=[\"device_id\", \"date_dt\"])\n",
    "\n",
    "\n",
    "# ## Add PHASE\n",
    "# df_hr =  add_phase_label_wrapper(df_hr, \"timestamp_dt\")  \n",
    "\n",
    "hr_input_feats = [\"hr_val\", \"out_of_range\", \"fat_burn\", \"cardio\", \"peak\"]\n",
    "\n",
    "# # Add week label\n",
    "# df_hr =  add_week_label(df_hr, \"date_dt\", df_survey)  \n",
    "\n",
    "\n",
    "\n",
    "## ADD ALL LABELS\n",
    "df_hr =  add_all_epoch_labels(df_hr, df_survey) \n",
    "df_hr_in_study = df_hr[(df_hr[\"is_during_study\"]==True)] # helps exclude incomplete days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a79d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_hr.head(2))\n",
    "print(df_hr.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ad4a6",
   "metadata": {},
   "source": [
    "### HR - intraday features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181595d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_hr_feats_per_day(df_grp):\n",
    "#     display(df_grp.head(2))\n",
    "    df_grp_out_of_range = df_grp[(df_grp[\"value_hr_zone\"]==\"Out of Range\")]\n",
    "    mean_hr_out_of_range = df_grp_out_of_range[\"value\"].mean()\n",
    "    mean_hr = df_grp[\"value\"].mean()\n",
    "    minutes_per_zone_dict = df_grp[\"value_hr_zone\"].value_counts().to_dict()\n",
    "    minutes_per_zone = defaultdict(int, minutes_per_zone_dict)\n",
    "    out_dict = {}\n",
    "    for z in [\"Out of Range\", \"Fat Burn\", \"Cardio\", \"Peak\"]:\n",
    "        out_dict[z] = minutes_per_zone[z]\n",
    "    out_dict[\"mean_hr\"] = mean_hr\n",
    "    out_dict[\"mean_hr_out_of_range\"] = mean_hr_out_of_range\n",
    "    return pd.Series(out_dict)\n",
    "\n",
    "\n",
    "\n",
    "def get_hr_intraday_feats_for_grp(df_grp):\n",
    "    feat_dict = {}\n",
    "#     display(df_grp.head(4))\n",
    "#     display(df_grp.iloc[132:137])\n",
    "    df_grp_per_day = df_grp.groupby(\"date_dt\").apply(get_hr_feats_per_day)\n",
    "    for f in [\"mean_hr\", \"mean_hr_out_of_range\", \"Out of Range\", \"Fat Burn\", \"Cardio\", \"Peak\"]:\n",
    "        feat_dict[f] = df_grp_per_day[f].mean()\n",
    "#     display(df_grp_per_day)\n",
    "#     display(feat_dict)\n",
    "    return pd.Series(feat_dict)\n",
    "    \n",
    "\n",
    "def get_hr_intraday_feats_for_did(did, df_survey, grpbycols, only_incld_during_study):\n",
    "    ## (1) did to fitbit id and load parsed csv\n",
    "    fid = FB_MAPPING_DICT_INV[did]\n",
    "    fpath = HR_INTRADAY_PATH_LIKE.format(fid)\n",
    "    data_df = pd.read_csv(fpath)\n",
    "    data_df[\"device_id\"] = did\n",
    "    data_df[\"timestamp_dt\"] = pd.to_datetime(data_df[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    data_df[\"date_dt\"] = data_df[\"timestamp_dt\"].dt.date\n",
    "    ## (2) add phase, tod, and dow lbls\n",
    "    data_df =  add_all_epoch_labels(data_df, df_survey, print_out=False) \n",
    "    if only_incld_during_study:\n",
    "        data_df = data_df[(data_df[\"is_during_study\"]==True)] # helps exclude incomplete days\n",
    "    #print (len(data_df))\n",
    "#     display(data_df.head(20))\n",
    "#     display(data_df.iloc[30000:30020])\n",
    "    ## (3) function to extract features from group\n",
    "    ## (4) grp by tod_lbl, and grp by tod_lbl and dow_lbl\n",
    "    feats_df_grp = data_df.groupby(grpbycols).apply(get_hr_intraday_feats_for_grp)\n",
    "    feats_df_grp = feats_df_grp.reset_index()\n",
    "#     print (\"features\")\n",
    "#     display(feats_df_grp)\n",
    "    return feats_df_grp\n",
    "    \n",
    "    \n",
    "def get_hr_intraday_feats(did_list, df_survey, grpbycols, only_incld_during_study):\n",
    "    did_df_list = []\n",
    "    cnt = 0\n",
    "    print (\"Processing did cnts intraday...\")\n",
    "    for did in did_list:\n",
    "        cnt += 1\n",
    "        print (\"{0}, \".format(cnt), end='')\n",
    "        did_df = get_hr_intraday_feats_for_did(did, df_survey, grpbycols, only_incld_during_study)\n",
    "        did_df_list.append(did_df)\n",
    "#         if cnt>5:\n",
    "#             break ## debugging\n",
    "    did_df_all = pd.concat(did_df_list)\n",
    "    return (did_df_all)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d2d61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tmp = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\"])\n",
    "\n",
    "# display (tmp.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9bdcab",
   "metadata": {},
   "source": [
    "### HR - Get per-phase/week features\n",
    "- We'll use ONLY intraday features. I\n",
    "gnore daily features completely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c339103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hr_phase_feats = df_hr.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# display(df_hr_phase_feats.head(2))\n",
    "# display(df_hr_phase_feats.isnull().sum(axis = 0))\n",
    "# # OUTPUT\n",
    "# output_features(df_hr_phase_feats, \"hr\", \"per_phase\")\n",
    "\n",
    "\n",
    "# # per-phase\n",
    "# df_hr_phase_feats = df_hr.groupby([\"device_id\", \"phase_wrapper\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_phase_feats, \"hr\", \"per_phase\")\n",
    "# # per-phase tod\n",
    "# df_hr_phase_feats = df_hr.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_phase_feats, \"hr\", \"per_phase_tod\")\n",
    "# # per-phase dow\n",
    "# df_hr_phase_feats = df_hr.groupby([\"device_id\", \"phase_wrapper\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_phase_feats, \"hr\", \"per_phase_dow\")\n",
    "# # per-phase tod, dow\n",
    "# df_hr_phase_feats = df_hr.groupby([\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_phase_feats, \"hr\", \"per_phase_tod_dow\")\n",
    "\n",
    "# per phase\n",
    "df_hr_phase_feats = get_hr_intraday_feats(list(df_hr_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\"], only_incld_during_study=True)\n",
    "output_features(df_hr_phase_feats, \"hr\", \"per_phase\")\n",
    "# per-phase tod\n",
    "df_hr_phase_feats = get_hr_intraday_feats(list(df_hr_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\"], only_incld_during_study=True)\n",
    "output_features(df_hr_phase_feats, \"hr\", \"per_phase_tod\")\n",
    "# per-phase dow\n",
    "df_hr_phase_feats = get_hr_intraday_feats(list(df_hr_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"dow_lbl\"], only_incld_during_study=True)\n",
    "output_features(df_hr_phase_feats, \"hr\", \"per_phase_dow\")\n",
    "# per-phase tod, dow\n",
    "df_hr_phase_feats = get_hr_intraday_feats(list(df_hr_in_study[\"device_id\"].unique()), df_survey, [\"device_id\", \"phase_wrapper\", \"tod_lbl\", \"dow_lbl\"], only_incld_during_study=True)\n",
    "output_features(df_hr_phase_feats, \"hr\", \"per_phase_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hr_biweekly_feats = df_hr.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# df_hr_monthly_feats = df_hr.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "\n",
    "# # biweekly\n",
    "# df_hr_biweekly_feats = df_hr.groupby([\"device_id\", \"biweekly_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_hr_biweekly_feats = df_hr.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_hr_biweekly_feats = df_hr.groupby([\"device_id\", \"biweekly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly_dow\")\n",
    "# # biweekly tod, dow\n",
    "# df_hr_biweekly_feats = df_hr.groupby([\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly_tod_dow\")\n",
    "\n",
    "# # biweekly\n",
    "# df_hr_biweekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"biweekly_lbl\"])\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly\")\n",
    "# # biweekly tod\n",
    "# df_hr_biweekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"biweekly_lbl\", \"tod_lbl\"])\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly_tod\")\n",
    "# # biweekly dow\n",
    "# df_hr_biweekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"biweekly_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly_dow\")\n",
    "# # biweekly tod dow\n",
    "# df_hr_biweekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"biweekly_lbl\", \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_hr_biweekly_feats, \"hr\", \"biweekly_tod_dow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b982d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # monthly\n",
    "# df_hr_monthly_feats = df_hr.groupby([\"device_id\", \"monthly_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_hr_monthly_feats = df_hr.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_hr_monthly_feats = df_hr.groupby([\"device_id\", \"monthly_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_hr_monthly_feats = df_hr.groupby([\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly_tod_dow\")\n",
    "\n",
    "# # monthly\n",
    "# df_hr_monthly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"monthly_lbl\"])\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly\")\n",
    "# # monthly tod\n",
    "# df_hr_monthly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"monthly_lbl\", \"tod_lbl\"])\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly_tod\")\n",
    "# # monthly dow\n",
    "# df_hr_monthly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"monthly_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly_dow\")\n",
    "# # monthly tod, dow\n",
    "# df_hr_monthly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"monthly_lbl\", \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly_tod_dow\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_hr_biweekly_feats.head(2))\n",
    "# display(df_hr_biweekly_feats.isnull().sum(axis = 0))\n",
    "# display(df_hr_monthly_feats.head(2))\n",
    "# display(df_hr_monthly_feats.isnull().sum(axis = 0))\n",
    "\n",
    "# # # OUTPUT\n",
    "# # output_features(df_hr_biweekly_feats, \"hr\", \"biweekly\")\n",
    "# output_features(df_hr_monthly_feats, \"hr\", \"monthly\")\n",
    "\n",
    "\n",
    "# # weekly\n",
    "# df_hr_weekly_feats = df_hr.groupby([\"device_id\", \"wknum_wrt_wk12\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_hr_weekly_feats = df_hr.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_hr_weekly_feats = df_hr.groupby([\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_hr_weekly_feats = df_hr.groupby([\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"]).apply((lambda x: add_features_mean(x, hr_input_feats))).reset_index()\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly_tod_dow\")\n",
    "\n",
    "\n",
    "# # weekly\n",
    "# df_hr_weekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"wknum_wrt_wk12\"])\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly\")\n",
    "# # weekly tod\n",
    "# df_hr_weekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\"])\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly_tod\")\n",
    "# # weekly dow\n",
    "# df_hr_weekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"wknum_wrt_wk12\", \"dow_lbl\"])\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly_dow\")\n",
    "# # weekly tod, dow\n",
    "# df_hr_weekly_feats = get_hr_intraday_feats(list(df_hr[\"device_id\"].unique()), df_survey, [\"device_id\", \"wknum_wrt_wk12\", \"tod_lbl\", \"dow_lbl\"])\n",
    "# output_features(df_hr_weekly_feats, \"hr\", \"weekly_tod_dow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c11af92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
